%----------------------------------------------------------------------------------------
%	CHAPTER 4
%----------------------------------------------------------------------------------------

\chapter{Introduction to Category Theory}

\epigraph{``In linear algebra, all the proofs should be straight-forward. There is no trick. If you think it's very hard, there is something wrong''}{Guowu Meng}

\section{Categories and Functors}

The collection of set maps is denoted as $\Set$ and the collection of linear maps over $\F$ is denoted as $\Vect_\F$.
There is a diagram below:
\begin{center}
	\begin{tikzcd}
		\Set \arrow[d, "{\F[-]}"] \\
		\Vect_\F
	\end{tikzcd}
\end{center}
where $\F[-]$ sends set map $f : X \to Y$ to a linear map $\F[f] : \F[X] \to \F[Y]$.

$\F[-]$ is an example of functors.

Monoid homomorphisms are another example of functors: in particular group homomorphisms
\begin{center}
	\begin{tikzcd}
		M_1 \arrow[d, "\phi"] \\
		M_2
	\end{tikzcd}
\end{center}

An element $a \in M_1$ is viewed as an arrow, or morphism, that sends $*$ to $*$, i.e., $a : * \to *$.
Then $ab$ is viewed as the composition of arrows:
\begin{center}
	\begin{tikzcd}
		* \arrow[r, "b"] \arrow[rr, "ab", bend right] & * \arrow[r, "a"] & *
	\end{tikzcd}
\end{center}

Recall that a monoid $M$ is a set, which is called a small collection of objects, together with a binary operation, which is also called composition, on $M$ with both the associtivity law and identity law satisfied.

By relaxing the condition on binary operation, allowing the composition being partially defined, we end up with the notion of \emph{small category}.

Being partially defined means that the composition may not be always defined.
For example, take $f : X \to Y$ and $g : W \to Z$, then $gf$ is not defined.
But for normal, $f : X \to Y$ and $g : Y \to Z$, then $gf$ is defined.
In monoid, as we may suggest there is only one element $*$, then the composition is always defined.

An example of small category: The collection of all matrices over $\F$.
We may consider any $m \times n$ matrix as an arrow that sends $n$ to $m$: $A : n \to m$.
If we have a $k \times m$ matrix $B$ that sends $m$ to $k$, then we have the composition $BA : n \to k$.
Note that $I_n : n \to n$ is the identity, which is not unique, there can be $I_m$ and $I_k$.
We have
\begin{center}
	\begin{tikzcd}
		n \arrow[loop left, "1_n"] \arrow[r, "A"] & m \arrow[loop right, "1_m"] \arrow[d, "B"] \\
		& k
	\end{tikzcd}
\end{center}
Note that $A 1_n = A = 1_m A$ and $B 1_m = B$.
\begin{remark}
	The identity elements are not unique unlike the case of monoid.
\end{remark}

The following shows the associativity law:
\begin{center}
	\begin{tikzcd}
		n \arrow[r, "C"] \arrow[rr, bend right, "BC"] \arrow[rrr, bend right, "A(BC)" swap] \arrow[rrr, bend left, "(AB)C"] & m \arrow[r, "B"] \arrow[rr, bend left, "AB" swap] & k \arrow[r, "A"] & l
	\end{tikzcd}
\end{center}

Hence, the set of all matrices form a small category.

Consider the set of all invertible matrices over $\F$, it is also a small category, in fact, it is a \emph{groupoid}\index{groupoid}.
Groupoid is defined as a small category such that every morphism is invertible.

\begin{center}
	\begin{tikzcd}[column sep=3.5em, row sep=huge, math mode=false]
		& Categories \\
		Monoids \arrow[r] & Small Categories \arrow[u] \\
		Groups \arrow[r] \arrow[u] & Groupoids \arrow[u]
	\end{tikzcd}
\end{center}
The graph above shows the relation, the arrows show the subsets relation.
The arrow head is the larger set and arrow tail is the subset.

\newpage

\section{Small Categories}

\begin{definition}[Small Categories]
	A small category is a set $\C$ together with a subset $\C_0$ of $\C$, two surjective maps $s, t : \C \to \C_0$ and a composition map $\C \times_{(s, t)} \C \to \C$ that sends $(f, g)$ to $fg$ which satisfies the identity law and associativity law.
\end{definition}

Here $\C \times_{s, t} \C$ is defined as the pullback of the diagram below:
\begin{center}
	\begin{tikzcd}
		\C \times_{s, t} \C \arrow[r, "p_1"] \arrow[d, "p_2"] \arrow[dr, phantom, "\ulcorner", very near start] & \C \arrow[d, "t"] \\
		\C \arrow[r, "s"] & \C_0
	\end{tikzcd}
\end{center}
where the set $\C \times_{s, t} \C = \{ (x, y) \in \C \times \C \mid s(x) = t(y) \}$.
Intuitively, the pullback is to filter out the mappings that can do composition, such as $f, g \in \C \times_{(s, t)} \C$ where \begin{tikzcd}[cramped, column sep=normal] A \arrow[r, "f"] & B \arrow[r, "g"] & C \end{tikzcd}.

The $s$ and $t$ are called the \emph{source map} and \emph{target map} respectively.
We can picture the composition graphically as follows:
\begin{center}
	\begin{tikzcd}[row sep=subtext, column sep=tiny]
		* & * \arrow[l, "f" swap] & * & * \arrow[l, "g" swap] \\
		t(f) & s(f) \arrow[r, equal] & t(g) & s(g)
	\end{tikzcd}
	\qquad
	\begin{tikzcd}[row sep=subtext, column sep=tiny]
		* & * \arrow[l, "fg" swap] \\
		t(f) & s(g)
	\end{tikzcd}
\end{center}
The left diagram is the equivalent to the right one.

We may draw the identity law this way:
\begin{center}
	\begin{tikzcd}[row sep=subtext, column sep=tiny]
		* \arrow[loop left, "1_{t(f)}"] & * \arrow[l, "f" swap] \\
		t(f) & s(f)
	\end{tikzcd}
	\qquad
	\begin{tikzcd}[row sep=subtext, column sep=tiny]
		* & * \arrow[l, "f" swap] \\
		t(f) & s(f)
	\end{tikzcd}
	\qquad
	\begin{tikzcd}[row sep=subtext, column sep=tiny]
		* & * \arrow[l, "f" swap] \arrow[loop right, "1_{s(f)}"] \\
		t(f) & s(f)
	\end{tikzcd}
\end{center}
The three diagrams are equivalent.

We may draw the associativity law this way:
\begin{center}
	\begin{tikzcd}[swap]
		* & * \arrow[l, "f"] & * \arrow[l, "g"] \arrow[ll, bend left, "fg", blue] & * \arrow[l, "h"] \arrow[lll, bend left, "(fg)h", blue] \arrow[ll, bend right, "gh" swap, red] \arrow[lll, bend right, "f(gh)" swap, red]
	\end{tikzcd}
\end{center}

\begin{example}
	In the small category of matrices over $\F$, we have
	\begin{align*}
		\C &= \{ \M{m \times n}{\F} \mid m, n \in \mathbb{N} \} \\
		\C_0 &= \{ I_n \mid n \in \mathbb{N} \} \equiv \mathbb{N}
	\end{align*}
	If $A \in \C$ is an $m \times n$ matrix, then $s(A) = I_n \equiv n$ and $t(A) = I_m \equiv m$.
	We can draw $A$ as follows:
	\begin{center}
		\begin{tikzcd}[row sep=subtext, swap]
			* & * \arrow[l, "A"] \\
			m & n
		\end{tikzcd}
	\end{center}
	Note that $(A, B) \in \C \times_{s, t} \C$, where the composition of $A$ and $B$ defined as the matrix multiplication $AB$, means for some positive integer $m, n$ and $k$:
	\begin{center}
		\begin{tikzcd}[row sep=subtext, swap]
			* & * \arrow[l, "A"] & * \arrow[l, "B"] \\
			m & n & k
		\end{tikzcd}
	\end{center}
\end{example}
\begin{remark}
	Elements in $\C$ are \emph{morphisms}\index{morphisms} or \emph{arrows}\index{arrows}, and elements in $\C_0$ are \emph{identity morphisms}.
	A morphism $f$ is viewed as an arrow from $s(f) \in C_0$ to $t(f) \in C_0$, i.e., $f : s(f) \to t(f)$.
	An identity morphism is drawn in the following way with $X$ being called the \emph{object}\index{object}:
	\begin{center}
		\begin{tikzcd}[row sep=subtext, swap]
			* & * \arrow[l, "1_X"] \\
			X & X
		\end{tikzcd}
	\end{center}
	In the last example, $I_n$ is the identity morphism at $n$.
	So $\C_0$ is also called the set of objects.
	Then a morphism $f$ is viewed as an arrow from object $X \equiv 1_X = s(f)$ to object $Y \equiv 1_Y = t(f)$, i.e., $f : X \to Y$.
\end{remark}

So, normally, we denote a small category as $\C$ and its set of objects as $\C_0$.
\begin{remark}
	The set of morphisms from object $X$ to object $Y$ is denoted by $\Mor(X, Y)$.
	In the last example, $\Mor(m, n) = \M{m \times n}{\F}$, the set of all $m \times n$ matrices over $\F$.
	Note that $1_X \in \Mor(X, X)$, so $\Mor(X, X) \neq \emptyset$ for all $X \in \C_0$.
\end{remark}

Then $\C$ is the disjoint union of all $\Mor(X, Y)$ for all pairs of objects $(X, Y)$:
\[
	\C = \bigsqcup_{X, Y \in \C_0} \Mor(X, Y)
\]
\begin{remark}
	The composition can be written as follows:
	\begin{center}
		\begin{tikzcd}[row sep=subtext]
			\Mor(Y, Z) \times \Mor(X, Y) \arrow[r] & \Mor(X, Z) \\
			(Z \xlongleftarrow{f} Y, X \xlongleftarrow{g} Y) \arrow[r, mapsto] & X \xlongleftarrow{fg} Z
		\end{tikzcd}
	\end{center}
\end{remark}

Then the following is the second definition of small category, which is also the normal definition of a small category.

\begin{definition}[Small Categories]
	A small category $\C$ is a collection of the following data:
	\begin{enumerate}
		\item A set of objects $\C_0$;
		\item A set of morphisms $\Mor(X, Y)$ for each pair of objects $(X, Y)$;
		\item A composition map $\Mor(Y, Z) \times \Mor(X, Y) \to \Mor(X, Z)$ that sends $(f, g)$ to $fg$ for each triple of objects $(X, Y, Z)$;
		\item An identity morphism $1_X \in \Mor(X, X)$ for each object $X$;
	\end{enumerate}
	Moreover, these data satisfies the following conditions:
	\begin{enumerate}[label=(\alph*)]
		\item (Identity Law) For all $f \in \Mor(X, Y)$, we have $f 1_X = f = 1_Y f$;
		\item (Associativity Law) For all appropriate morphisms $f, g, h$, we have $(fg)h = f(gh)$.
	\end{enumerate}
\end{definition}

For a small category $\C$, the set of objects is denoted by $\Ob(\C)$ and the set of morphisms for any pair of objects $(X, Y)$ is denoted by $\Mor(X, Y)$, $\Mor_\C (X, Y)$, Hom$_\C(X, Y)$ or simply $\C(X, Y)$.

If we allow $\Ob(\C)$ and $\Mor_\C(X, Y)$ for any pair of objects $(X, Y)$ being a \emph{class}\index{class}, (a larger collection than set), we end up with the definition of \emph{category}\index{category}.

We say a morphism is \emph{isomorphic}\index{isomorphic} or \emph{invertible}\index{invertible} if it has a two-sided inverse.
A category such that every morphism is isomorphic is called a \emph{groupoid}\index{groupoid}.

\begin{example}
	The collection of all sets and set maps, denoted by $\Set$, is a category.
\end{example}

\begin{example}
	The collection of all linear spaces over $\F$ and linear maps, denoted by $\Vect_\F$, is a category.
\end{example}

\begin{example}
	If $\C$ and $\D$ are two categories, then we have the product category $\C \times \D$ with objects $(X, Y)$ and morphisms $(f, g)$, where $X \in \Ob(\C)$, $Y \in \Ob(\D)$, $f \in \Mor_\C(X, X')$ and $g \in \Mor_\D(Y, Y')$.
\end{example}

\begin{example}
	The category of set maps between finite sets, denoted by $\boldsymbol{\mathsf{FinSet}}$, is a subcategory of $\Set$.
\end{example}

\begin{example}
	Fix an object $X$ in a category $\C$.
	Then the collection of all morphisms with source $X$, denoted by $\C(X, -)$, is a new category:
	\begin{itemize}
		\item Objects: all morphisms $f : X \to Y$ in $\C$ for all $Y \in \Ob(\C)$;
		\item Morphisms: commutative triangles in $\C$:
		\begin{center}
			\begin{tikzcd}
				& X \arrow[dl, "f" swap] \arrow[dr, "f'"] \\
				Y \arrow[rr, "g"] & & Y'
			\end{tikzcd}
		\end{center}
		\item The identity morphism at object $f : X \to Y$ is the commutative triangle in $\C$:
		\begin{center}
			\begin{tikzcd}
				& X \arrow[dl, "f" swap] \arrow[dr, "f"] \\
				Y \arrow[rr, "1_Y"] & & Y
			\end{tikzcd}
		\end{center}
	\end{itemize}
\end{example}

\begin{example}
	Let $V$ be a subspace of the linear space $W$ over $\F$.
	Then we have a category:
	\begin{itemize}
		\item Objects: all morphisms $f : W \to Z$ in $\Vect_\F$ such that $f\mid_V = 0$;
		\item Morphisms: commutative triangles in $\Vect_\F$:
		\begin{center}
			\begin{tikzcd}
				& W \arrow[dl, "f_1" swap] \arrow[dr, "f_2"] \\
				Z_1 \arrow[rr, "g"] & & Z_2
			\end{tikzcd}
		\end{center}
	\end{itemize}
\end{example}

\begin{definition}[Terminal Object and Initial Object]
	Let $\C$ be a category.
	An object $T \in \Ob(\C)$ is called a \emph{terminal object} if for all object $X$, there exists a unique morphism from $X$ to $T$, i.e., $|\C(X, T)| = 1$.
	An object $I \in \Ob(\C)$ is called an \emph{initial object} if for all object $X$, there exists a unique morphism from $I$ to $X$, i.e., $|\C(I, X)| = 1$.
\end{definition}

\begin{corollary}
	A terminal object or an initial object is unique up to isomorphism.
\end{corollary}

\begin{example}
	In the last example of category, the quotient map $\pi : W \to \quotient{W}{V}$ is an initial object and the zero map $0 : W \to 0$ is a terminal object.
\end{example}

\begin{example}
	In $\Set$, any singleton set is a terminal object, and the empty set is an initial object.
\end{example}

\begin{example}
	In $\Vect_\F$, the zero vector space is both a terminal object and an initial object.
\end{example}

\newpage

\section{Products and Coproducts}

\subsection{Products}

\begin{definition}[Products]
	Let $\C$ be a category and $X, Y \in \Ob(\C)$.
	The \emph{product}\index{product} of $X$ and $Y$ is an object $X \prod Y$ together with two morphisms $\pi_X : X \prod Y \to X$ and $\pi_Y : X \prod Y \to Y$ such that for any object $Z$ and any two morphisms $f_X : Z \to X$ and $f_Y : Z \to Y$, there exists a unique morphism $f : Z \to X \prod Y$ such that the following diagram commutes:
	\begin{center}
		\begin{tikzcd}
			& Z \arrow[dl, "f_X" swap] \arrow[d, dashed, "\exists ! f"] \arrow[dr, "f_Y"] \\
			X & X \prod Y \arrow[l, "\pi_X"] \arrow[r, "\pi_Y" swap] & Y
		\end{tikzcd}
	\end{center}
\end{definition}
\begin{remark}
	The product is unique up to isomorphism if it exists.
\end{remark}

\begin{corollary}
	Let $\C$ be a category and $X, Y \in \Ob(\C)$.
	Consider the following new category:
	\begin{itemize}
		\item Objects: all morphisms \begin{tikzcd}[cramped, swap, column sep=normal] X \arrow[from=r, "f_X"] & Z & Y \arrow[from=l, "f_Y" swap] \end{tikzcd} in $\C$ for all $Z \in \Ob(\C)$;
		\item Morphisms: commutative diagrams in $\C$:
	\end{itemize}
	\begin{center}
		\begin{tikzcd}[row sep=normal]
			& Z \arrow[dd, "f"] \arrow[dl, "f_X" swap] \arrow[dr, "f_Y"] \\
			X & & Y \\
			& Z' \arrow[ul, "f_X'" swap] \arrow[ur, "f_Y'"]
		\end{tikzcd}
	\end{center}
	Then the product of $X$ and $Y$ is a terminal object in this new category.
\end{corollary}

\begin{example}
	In $\Set$, the product of two sets $X$ and $Y$ is the Cartesian product $X \times Y = \{ (x, y) \mid x \in X, y \in Y \}$ with the projection maps $\pi_X(x, y) = x$ and $\pi_Y(x, y) = y$.
	Then with $f(z) = (f_X(z), f_Y(z))$ for all $z \in Z$, we have the following commutative diagram:
	\begin{center}
		\begin{tikzcd}
			& Z \arrow[dl, "f_X" swap] \arrow[d, dashed, "\exists ! f"] \arrow[dr, "f_Y"] \\
			X & X \times Y \arrow[l, "\pi_X"] \arrow[r, "\pi_Y" swap] & Y
		\end{tikzcd}
	\end{center}

\end{example}

\begin{example}
	In $\Vect_\F$, the product of two linear spaces $V_1$ and $V_2$ over $\F$ is the direct product $V_1 \times V_2 = \{ (v_1, v_2) \mid v_1 \in V_1, v_2 \in V_2 \}$ with the projection maps $\pi_{V_1}(v_1, v_2) = v_1$ and $\pi_{V_2}(v_1, v_2) = v_2$.
	Then with $f(z) = (f_{V_1}(z), f_{V_2}(z))$ for all $z \in Z$, we have the following commutative diagram:
	\begin{center}
		\begin{tikzcd}
			& Z \arrow[dl, "f_{V_1}" swap] \arrow[d, dashed, "\exists ! f"] \arrow[dr, "f_{V_2}"] \\
			V_1 & V_1 \times V_2 \arrow[l, "\pi_{V_1}"] \arrow[r, "\pi_{V_2}" swap] & V_2
		\end{tikzcd}
	\end{center}
\end{example}

\subsection{Coproducts}

\begin{definition}[Coproducts]
	Let $\C$ be a category and $X, Y \in \Ob(\C)$.
	The \emph{coproduct}\index{coproduct} of $X$ and $Y$ is an object $X \coprod Y$ together with two morphisms $\iota_X : X \to X \coprod Y$ and $\iota_Y : Y \to X \coprod Y$ such that for any object $Z$ and any two morphisms $f_X : X \to Z$ and $f_Y : Y \to Z$, there exists a unique morphism $f : X \coprod Y \to Z$ such that the following diagram commutes:
	\begin{center}
		\begin{tikzcd}
			X \arrow[r, "\iota_X"] \arrow[dr, "f_X" swap] & X \coprod Y \arrow[d, dashed, "\exists ! f"] & Y \arrow[l, "\iota_Y" swap] \arrow[dl, "f_Y"] \\
			& Z
		\end{tikzcd}
	\end{center}
\end{definition}
\begin{remark}
	The coproduct is unique up to isomorphism if it exists.
\end{remark}

\begin{corollary}
	Let $\C$ be a category and $X, Y \in \Ob(\C)$.
	The \emph{coproduct}\index{coproduct} of $X$ and $Y$ is the initial object in the new category:
	\begin{itemize}
		\item Objects: all morphisms \begin{tikzcd}[cramped, column sep=normal] X \arrow[r, "f_X"] & Z & Y \arrow[l, "f_Y" swap] \end{tikzcd} in $\C$ for all $Z \in \Ob(\C)$;
		\item Morphisms: commutative diagrams in $\C$:
	\end{itemize}
	\begin{center}
		\begin{tikzcd}[row sep=normal]
			& Z \\
			X \arrow[ur, "f_X"] \arrow[dr, "f_X'" swap] & & Y \arrow[ul, "f_Y" swap] \arrow[dl, "f_Y'"] \\
			& Z' \arrow[uu, "f"]
		\end{tikzcd}
	\end{center}
\end{corollary}

\begin{example}
	In $\Set$, the coproduct of two sets $X$ and $Y$ is the disjoint union $X \sqcup Y = \{ (x, 1) \mid x \in X \} \cup \{ (y, 2) \mid y \in Y \}$.
\end{example}

\begin{example}
	In $\Vect_\F$, the coproduct of two linear spaces $V_1$ and $V_2$ over $\F$ is the direct sum $V_1 \oplus V_2 = \{ (v_1, v_2) \mid v_1 \in V_1, v_2 \in V_2 \}$.
\end{example}

\subsection{Biproducts}

In $\Vect_\F$, the product and coproduct are the same, i.e., $V_1 \times V_2 \cong V_1 \oplus V_2$.
Then we will say the \emph{biproduct}\index{biproduct} of $V_1$ and $V_2$ and denote it by $V_1 \oplus V_2$.
The following diagram commutes:
\begin{center}
	\begin{tikzcd}[row sep=normal]
		& V_1 \times V_2 \arrow[dl, "\pi_{V_1}" swap] \arrow[dr, "\pi_{V_2}"] \\
		V_1 \arrow[dr, "\iota_{V_1}"] && V_2 \arrow[dl, "\iota_{V_2}" swap] \\
		& V_1 \oplus V_2 \arrow[uu, equal]
	\end{tikzcd}
\end{center}

\begin{definition}[Biproducts]
	The \emph{biproduct}\index{biproduct} of two objects $X$ and $Y$ in a category $\C$ is an object $X \oplus Y$ that is both the product and coproduct of $X$ and $Y$.
\end{definition}
\begin{remark}
	The biproduct exists if and only if the product and coproduct exist and are isomorphic, or if the initial object and the terminal object exist and are isomorphic.
\end{remark}

\begin{example}
	In $\Vect_\F$, the zero vector space is both a terminal object and an initial object, so the biproduct exists.
\end{example}

However, in $\Set$, the empty set is an initial object but the terminal object is any singleton set, so the biproduct does not exist.

\subsection{Products and Coproducts of a Family of Objects}

In general, we may have the product or coproduct of a family of objects.

Let $\C$ be a category and $\{ X_\alpha \}_{\alpha \in I}$ be a collection of objects in $\C$ indexed by a set $I$, called the \emph{indexing set}.
The \emph{product}\index{product} of $\{ X_\alpha \}_{\alpha \in I}$ is the terminal object in the new category:
\begin{itemize}
	\item Objects: all collections of morphisms $\{ f_\alpha : Z \to X_\alpha \}_{\alpha \in I}$ in $\C$ for all $Z \in \Ob(\C)$;
	\item Morphisms: for all $\alpha \in I$, commutative diagrams in $\C$:
\end{itemize}
\begin{center}
	\begin{tikzcd}[column sep=normal]
		& X_\alpha \\
		Z \arrow[ur, "f_\alpha"] \arrow[rr, "f" swap, dashed] & & Z' \arrow[ul, "f_\alpha'" swap]
	\end{tikzcd}
\end{center}

The \emph{coproduct}\index{coproduct} of $\{ X_\alpha \}_{\alpha \in I}$ is the initial object in the new category:
\begin{itemize}
	\item Objects: all collections of morphisms $\{ f_\alpha : X_\alpha \to Z \}_{\alpha \in I}$ in $\C$ for all $Z \in \Ob(\C)$;
	\item Morphisms: for all $\alpha \in I$, commutative diagrams in $\C$:
\end{itemize}
\begin{center}
	\begin{tikzcd}[column sep=normal]
		& Z \arrow[dr, "f_\alpha"] \arrow[dl, "f_\alpha'" swap] \\
		X_\alpha & & Z' \arrow[ll, "f", dashed]
	\end{tikzcd}
\end{center}

Then the product and coproduct have the following universal properties respectively:
\begin{center}
	\begin{tikzcd}
		X_\alpha & Z \arrow[dl, "\exists ! f"] \arrow[l, "\forall f_\alpha" swap, dashed] \\
		\prod X_\alpha \arrow[u, "\pi_\alpha"]
	\end{tikzcd}
	\qquad
	\begin{tikzcd}[swap]
		X_\alpha \arrow[d, "\iota_\alpha"] \arrow[r, "\forall f_\alpha" swap] & Z \\
		\coprod X_\alpha \arrow[ur, "\exists ! f", dashed]
	\end{tikzcd}
\end{center}

The elements in the product of a family of objects in $\Vect_\F$ can be written as ordered tuples: $(v_\alpha)_{\alpha \in I}$.
Then the product can be defined as follows:
\[
	\prod_{\alpha \in I} V_\alpha = \{ (v_\alpha)_{\alpha \in I} \mid v_\alpha \in V_\alpha \}
\]
Then the coproduct can be defined as follows:
\[
	\bigoplus_{\alpha \in I} V_\alpha = \{ (v_\alpha) \in \prod_{\alpha \in I} V_\alpha \mid v_\alpha \text{ is finitely supported} \} \subseteq \prod_{\alpha \in I} V_\alpha
\]
\begin{remark}
	In general, the product is not equal to the coproduct.
	They are equal if and only if the indexing set $I$ is a finite set.
\end{remark}

Consider the following diagram:
\begin{center}
	\begin{tikzpicture}
		\filldraw[gray!10] (-4,-2) rectangle (4,2);
		\draw[step=0.5cm,gray!20] (-4,-2) grid (4,2);

		\draw[red] plot [smooth] coordinates { (-4, 1.5) (-2.5, 1) (-1, -1) (0.5, -1) (2, -1.5) (4, -2) };
		\draw[ocre] plot [smooth] coordinates { (-4, -0.5) (-2.5, 0) (-1, 0) (0.5, 1.5) (2, 0) (4, 0) };

		\draw[thick] (-2.5,-2) -- (-2.5,2) node [above] {\scriptsize $V_{\alpha_1}$};
		\draw[thick] (-1,-2)  -- (-1,2) node [above] {\scriptsize $V_{\alpha_2}$};
		\draw[thick] (0.5,-2) -- (0.5,2) node [above] {\scriptsize $V_{\alpha_3}$};
		\draw[thick] (2,-2)  -- (2,2) node [above] {\scriptsize $V_{\alpha_4}$};

		\path (3.5, 2) node [above] {\scriptsize $\cdots$};

		\filldraw (-2.5, 1) circle (1pt) node [above right] {\scriptsize $s_1(\alpha_1)$};
		\filldraw (-1, -1) circle (1pt) node [above right] {\scriptsize $s_1(\alpha_2)$};
		\filldraw (0.5, -1) circle (1pt) node [above right] {\scriptsize $s_1(\alpha_3)$};
		\filldraw (2, -1.5) circle (1pt) node [above right] {\scriptsize $s_1(\alpha_4)$};

		\filldraw (0.5, 1.5) circle (1pt) node [above right] {\scriptsize $s_2(\alpha_3)$};

		\filldraw (-2.5, 0) circle (1pt) node [below right] {\scriptsize $0_{\alpha_1}$};
		\filldraw (-1, 0) circle (1pt) node [below right] {\scriptsize $0_{\alpha_2}$};
		\filldraw (0.5, 0) circle (1pt) node [below right] {\scriptsize $0_{\alpha_3}$};
		\filldraw (2, 0) circle (1pt) node [below right] {\scriptsize $0_{\alpha_4}$};

		\draw[decoration={brace,raise=5pt},decorate]
			(-4,-2) -- node [left=6pt] {$\displaystyle \bigcup_{\alpha \in I} V_\alpha$} (-4,2);
		\draw[-Stealth] (-4 cm - 22 pt, -4 cm + 10 pt) -- (-4 cm - 22 pt,-15 pt) node [midway, left] {{\color{red} $s_1$}, {\color{ocre} $s_2$}};

		\draw[thick] (-4,-4) node [left=16pt] {$I$} -- (4,-4);

		\draw[dashed] (-2.5,-2) -- (-2.5,-4);
		\draw[dashed] (-1,-2) -- (-1,-4);
		\draw[dashed] (0.5,-2) -- (0.5,-4);
		\draw[dashed] (2,-2) -- (2,-4);

		\filldraw (-2.5,-4) circle (1.5pt) node [below] {$\alpha_1$};
		\filldraw (-1,-4) circle (1.5pt) node [below] {$\alpha_2$};
		\filldraw (0.5,-4) circle (1.5pt) node [below] {$\alpha_3$};
		\filldraw (2,-4) circle (1.5pt) node [below] {$\alpha_4$};
	\end{tikzpicture}
\end{center}
\begin{remark}
	The right sections $s_1$ and $s_2$ are two elements in the product $\prod V_\alpha$.
	Note that $s_2$ is likely to be ``finitely supported'' since it is zero in almost all components shown in the diagram.
	However, if $I$ is an infinite set, then $s_2$ may not be finitely supported since there may be infinitely many non-zero components not shown in the diagram.
	So $s_2$ may not be an element in the coproduct $\bigoplus V_\alpha$ if $I$ is an infinite set, but most likely to be.
\end{remark}

So the product $\prod V_\alpha$ contains all possible sections $s : I \to \bigcup V_\alpha$, so it is called the \emph{space of sections}.
The coproduct $\bigoplus V_\alpha$ contains all finitely supported sections, so it is called the \emph{space of sections with finite support}.
The elements in the coproduct $\bigoplus V_\alpha$ written as ordered tuples $(v_\alpha)_{\alpha \in I}$ can also be written as finite sums $\sum_{\alpha \in I} v_\alpha$ since only finitely many $v_\alpha$ are non-zero.

Actually, the product and coproduct are the generalisation of the polynomial ring and the formal power series ring respectively.
We can consider the following diagrams:
\begin{center}
	\begin{tikzpicture}
		\filldraw[gray!10] (-4,-2) rectangle (4,2);
		\draw[step=0.5cm,gray!20] (-4,-2) grid (4,2);

		\draw[red] plot [smooth] coordinates { (-4, 1.5) (-2.5, 1) (-1, -1) (0.5, 1.5) (2, -1.5) (4, -0.5) };

		\filldraw (-2.5, 1) circle (1pt) node [above right] {\scriptsize $s(\alpha_1)$};
		\filldraw (-1, -1) circle (1pt) node [right] {\scriptsize $s(\alpha_2)$};
		\filldraw (0.5, 1.5) circle (1pt) node [right] {\scriptsize $s(\alpha_3)$};
		\filldraw (2, -1.5) circle (1pt) node [below right] {\scriptsize $s(\alpha_4)$};

		\draw[thick] (-2.5,-2) -- (-2.5,2) node [above] {\scriptsize $\F$};
		\draw[thick] (-1,-2)  -- (-1,2) node [above] {\scriptsize $\F$};
		\draw[thick] (0.5,-2) -- (0.5,2) node [above] {\scriptsize $\F$};
		\draw[thick] (0.5,-2) -- (0.5,2) node [above] {\scriptsize $\F$};
		\draw[thick] (2,-2)  -- (2,2) node [above] {\scriptsize $\F$};

		\path (3.5, 2) node [above] {\scriptsize $\cdots$};

		\draw[decoration={brace,raise=5pt},decorate]
			(-4,-2) -- node [left=6pt] {$\F[S]$} (-4,2);
		\draw[-Stealth] (-4 cm - 20 pt, -4 cm + 10 pt) -- (-4 cm - 20 pt,-10 pt) node [midway, left] {$s$};

		\draw[thick] (-4,-4) node [left=13pt] {$S$} -- (4,-4);

		\draw[dashed] (-2.5,-2) -- (-2.5,-4);
		\draw[dashed] (-1,-2) -- (-1,-4);
		\draw[dashed] (0.5,-2) -- (0.5,-4);
		\draw[dashed] (2,-2) -- (2,-4);

		\filldraw (-2.5,-4) circle (1.5pt) node [below] {$\alpha_1$};
		\filldraw (-1,-4) circle (1.5pt) node [below] {$\alpha_2$};
		\filldraw (0.5,-4) circle (1.5pt) node [below] {$\alpha_3$};
		\filldraw (2,-4) circle (1.5pt) node [below] {$\alpha_4$};

		\draw[thick] (6, -2) -- (6, 2) node [above] {\scriptsize $\F$};

		\filldraw (6, 1) circle (1pt) node [right] {\scriptsize $s(\alpha_1)$};
		\filldraw (6, -1) circle (1pt) node [right] {\scriptsize $s(\alpha_2)$};
		\filldraw (6, 1.5) circle (1pt) node [right] {\scriptsize $s(\alpha_3)$};
		\filldraw (6, -1.5) circle (1pt) node [right] {\scriptsize $s(\alpha_4)$};
	\end{tikzpicture}
\end{center}

The left shows the diagram in generalised version, but it can be squeezed to the right since all fibres are the same.
So we can consider the set map as $s : S \to \F$ as shown on the right.

\newpage

\section{Functors}

\begin{definition}[Functors]
	Let $\C$ and $\D$ be two categories.
	A \emph{functor}\index{functor} $F : \C \to \D$ consists of the following data:
	\begin{itemize}
		\item A map $F : \Ob(\C) \to \Ob(\D)$;
		\item A map $F : \Mor_\C(X, Y) \to \Mor_\D(F(X), F(Y))$ for all $X, Y \in \Ob(\C)$;
	\end{itemize}
	such that the following conditions are satisfied:
	\begin{enumerate}[label=(\alph*)]
		\item For all $X \in \Ob(\C)$, we have $F(1_X) = 1_{F(X)}$;
		\item For all appropriate morphisms $f, g$ in $\C$, we have $F(fg) = F(f) F(g)$.
	\end{enumerate}
\end{definition}

\begin{example}
	There are two functors from $\Set$ to $\Vect_\F$:
	\begin{center}
		\begin{tikzcd}
			\Set \arrow[r, "{\F[-]}", yshift=0.5ex] & \Vect_\F \arrow[l, "|-|", yshift=-0.5ex]
		\end{tikzcd}
	\end{center}
	where $\F[-]$ sends set $X$ to the free vector space $\F[X]$ generated by $X$, and a set map $f : X \to Y$ to the linear map $\F[f] : \F[X] \to \F[Y]$ induced by $f$.
	The functor $|-|$ sends a vector space $V$ to its underlying set $|V|$, and a linear map $\phi : V \to W$ to the set map $|\phi| : |V| \to |W|$ induced by $\phi$.

	The $\F[-]$ is called the \emph{free functor}, specifically it is the \emph{free vector space functor}.
	The $|-|$ is called the \emph{underlying functor} or \emph{forgetful functor}.
\end{example}

For some set $X$ and any vector space $V$, we can consider the following diagram:
\begin{center}
	\begin{tikzcd}
		X \arrow[r, "\forall \phi"] \arrow[d, "\iota", hook] & V \\
		\F[X] \arrow[ur, dashed, "\exists ! \bar{\phi}" swap]
	\end{tikzcd}
\end{center}
This is called the \emph{universal property of free vector space over a set}.
Here $\iota : X \to \F[X]$ is the inclusion map, $\phi : X \to V$ is any set map, and $\bar{\phi} : \F[X] \to V$ is the unique linear map induced by $\phi$.
\begin{remark}
	The universal property of free vector space over a set can be rephrased as follows: for any set $X$ and any vector space $V$, there is a natural identification:
	\[
		\Set(X, |V|) \equiv \Vect_\F(\F[X], V)
	\]
	where $\Set(X, |V|)$ is the set of all set maps from $X$ to the underlying set of $V$, and $\Vect_\F(\F[X], V)$ is the set of all linear maps from the free vector space $\F[X]$ to $V$.

	If we consider $\phi : X \to |V|$ as an element in $\Set(X, |V|)$, then the corresponding element in $\Vect_\F(\F[X], V)$ is the unique linear map $\bar{\phi} : \F[X] \to V$ induced by $\phi$.

	Note that $\iota \equiv 1_{\F[X]}$ is the identity element in $\Vect_\F(\F[X], \F[X])$, so it corresponds to an element in $\Set(X, |\F[X]|)$, which is exactly the inclusion map $\iota : X \to |\F[X]|$.
\end{remark}

\begin{definition}[Adjoint Functors]
	Let $\C$ and $\D$ be two categories.
	A functor $F : \C \to \D$ is called a \emph{left adjoint} of a functor $G : \D \to \C$, and $G$ is called a \emph{right adjoint} of $F$, if there is a natural identification:
	\[
		\D(F(X), Y) \equiv \C(X, G(Y))
	\]
	for all $X \in \Ob(\C)$ and $Y \in \Ob(\D)$.
\end{definition}

\begin{example}
	The free functor $\F[-] : \Set \to \Vect_\F$ is a left adjoint of the underlying functor $|-| : \Vect_\F \to \Set$.
	This is exactly the universal property of free vector space over a set.
\end{example}

\begin{definition}[Endofunctors]
	An \emph{endofunctor}\index{endofunctor} is a functor $F : \C \to \C$ that maps a category to itself.
\end{definition}

\begin{example}
	Let $X$ be a set.
	Then we have an adjoint pair of functors:
	\begin{center}
		\begin{tikzcd}
			\Set \arrow[r, "- \times X", yshift=0.5ex] & \Set \arrow[l, "{\Set(X, -)}", yshift=-0.5ex]
		\end{tikzcd}
	\end{center}

	On the left is the endofunctor $- \times X$ and on the right is the endofunctor $\Set(X, -)$.
	\begin{center}
		\begin{tikzcd}[row sep=normal]
			\Set \arrow[r, "- \times X"] & \Set \\[-1.8em]
			Y \arrow[dd, "f" swap] & Y \times X \arrow[dd, "f \times 1_X"] \\
			\arrow[r, mapsto, shorten <= 2ex, shorten >= 2ex] & \phantom{*} \\
			Z & Z \times X
		\end{tikzcd}
		\qquad\qquad
		\begin{tikzcd}[row sep=normal]
			\Set & \Set \arrow[l, "{\Set(X, -)}" swap] \\[-1.8em]
			\Set(X, Y) \arrow[dd, "{\Set(X, f)}" swap] & Y \arrow[dd, "f"] \\
			\phantom{*} & \arrow[l, mapsto, shorten <= 2ex, shorten >= 2ex] \\
			\Set(X, Z) & Z
		\end{tikzcd}
	\end{center}

	Consider an element $g \in \Set(X, Y)$, which is a set map $g : X \to Y$.
	Then the corresponding element in $\Set(X, Z)$ is $\Set(X, f)(g) = fg : X \to Z$.

	Then we can write the natural identification as follows:
	\[
		\Set(Y \times X, Z) \equiv \Set(Y, \Set(X, Z))
	\]
	for all sets $Y$ and $Z$.
	This means that a set map $F : Y \times X \to Z$ corresponds to a set map $F_{\musNatural} : Y \to \Set(X, Z)$ such that a $y \in Y$ is mapped to a set map $F_{\musNatural}(y) : X \to Z$ defined by $F_{\musNatural}(y)(x) = F(y, x)$ for all $x \in X$.
\end{example}

Consider the following two diagrams:
\begin{center}
	\begin{tikzcd}[column sep=normal]
		X_1 & X_1 \times X_2 \arrow[l] \arrow[r] \arrow[d, "{\F[-]}", Rightarrow] & X_2 \\
		\F[X_1] & \F[X_1 \times X_2] \arrow[l] \arrow[r] & \F[X_2] \\[-3.6em]
		& {\scriptstyle \equiv \F[X_1] \otimes \F[X_2]}
	\end{tikzcd}
	\qquad
	\begin{tikzcd}[column sep=normal]
		X_1 \arrow[r] & X_1 \sqcup X_2 \arrow[d, "{\F[-]}", Rightarrow] & X_2 \arrow[l] \\
		\F[X_1] \arrow[r] & \F[X_1 \sqcup X_2] & \F[X_2] \arrow[l] \\[-3.6em]
		& {\scriptstyle \equiv \F[X_1] \oplus \F[X_2]}
	\end{tikzcd}
\end{center}
The left diagram shows that the free functor sends the product of two sets to the tensor product of two vector spaces.
The right diagram shows that the free functor sends the coproduct of two sets to the direct sum of two vector spaces, i.e., the coproduct of two vector spaces.
Note that the tensor product of two vector spaces is \emph{not}\index{not} the product of two vector spaces, as the dimension of the tensor product is $\dim(V_1 \otimes V_2) = \dim(V_1) \cdot \dim(V_2)$ while the dimension of the product is $\dim(V_1 \oplus V_2) = \dim(V_1) + \dim(V_2)$.
There is a unique but not isomorphic linear map $\phi : V_1 \otimes V_2 \to V_1 \oplus V_2$.
\begin{remark}
	The left adjoint functor preserves coproducts, and the right adjoint functor preserves products.
	This is the consequences of the \emph{adjoint functor theorem}.
\end{remark}

Similarly, we have the following natural identifications:
\[
	\Vect_\F(X \otimes Y, Z) \equiv \Vect_\F(Y, \Vect_\F(X, Z))
\]
Note that $\Vect_\F(X, Z)$ is a vector space over $\F$, as $\Vect_\F \equiv \Hom_\F$.
Then, we have the following adjoint pair of endofunctors on $\Vect_\F$:
\begin{center}
	\begin{tikzcd}
		\Vect_\F \arrow[r, "- \otimes X", yshift=0.5ex] & \Vect_\F \arrow[l, "{\Hom_\F(X, -)}", yshift=-0.5ex]
	\end{tikzcd}
\end{center}

\newpage

\section{Dual Spaces and Dual Bases}

Let $V$ be a finite dimensional linear space over $\F$.
The \emph{dual space} of $V$ is the vector space $V^* = \Hom_\F(V, \F)$, the set of all linear functionals from $V$ to $\F$, or \emph{covectors}\index{covectors}.

\begin{proposition}
	Let $V$ be a finite dimensional linear space over $\F$.
	Then $\dim(V^*) = \dim(V)$.
	So, $V^*$ is isomorphic to $V$ but not naturally isomorphic to $V$.
\end{proposition}
\begin{proof}
	Without the loss of generality, we may assume $\dim{V} = n$ and $V = \F^n$.
	Then $V^* = \Hom_\F(\F^n, \F) \equiv \M{1 \times n}{\F}$, the linear space of row matrices with $n$ entries.
	The linear space is the span of $n$ standard basis row matrices: $\hat{e}^1, \hat{e}^2, \cdots, \hat{e}^n$.
	So $\dim(V^*) = n = \dim(V)$.
	We can say $V^* \cong V$.
\end{proof}

We have a map $\phi_s : \F^n \to (\F^n)^* \supset S = \{\hat{e}^1, \hat{e}^2, \cdots, \hat{e}^n\}$ defined by $\phi_s(\vec{x}) = \sum_{i=1}^n x_i \hat{e}^i$.
This is a vector space isomorphism but not a natural isomorphism, as it depends on the choice of $S$.

\begin{definition}[Bases]
	A \emph{basis}\index{basis} of a linear space $V$ over $\F$ is the minimal spanning set of $V$ with an order.
	The set of all bases of $V$ is denoted by $\B_V$.
\end{definition}

\begin{proposition}
	$\B_V$ and $\B_{V^*}$ are naturally isomorphic in $\Set$, i.e., the following natural identification holds:
	\begin{align*}
		\B_V &\equiv \B_{V^*} \\
		v = (\vec{v}_1, \vec{v}_2, \cdots, \vec{v}_n) &\equiv (\hat{v}^1, \hat{v}^2, \cdots, \hat{v}^n) = v^* \\
	\end{align*}
	where $\hat{v}^i \in V^*$ is defined by $\hat{v}^i(\vec{v}_j) = \delta^i_j$ for all $1 \leq i, j \leq n$.
\end{proposition}

\begin{proof}
Consider the following commutative diagram:
\begin{center}
	\begin{tikzcd}
		V \arrow[d, "{[-]_V}" swap, hook, two heads] \arrow[dr, dashed, "\hat{v_i}"] \\
		\F^n \arrow[r, "\pi_i"] & \F
	\end{tikzcd}
\end{center}
The projection map $\pi_i$ is a linear functional in $\F^n$ that sends $\vec{x} = (x_1, x_2, \cdots, x_n)$ to $x_i$.
It is actually $\hat{e}^i$.
Note that $[-]_V : V \to \F^n$ is a coordinate map defined by a basis $v = (\vec{v}_1, \vec{v}_2, \cdots, \vec{v}_n) \in \B_V$ such that $[\vec{v}_j]_V = \vec{e}_j$ for all $1 \leq j \leq n$.
It is a unique linear map which identify $\vec{v}_i$ with $\vec{e}_i$.
It can be done by trivialisation of $V$ with respect to the basis $v$.
Then we define $\hat{v}^i(\vec{v}_j) = \delta^i_j$ for all $1 \leq i, j \leq n$.

Then we have to consider whether $(\hat{v}^1, \hat{v}^2, \cdots, \hat{v}^n)$ is a basis of $V^*$.
As $\dim{V^*} = n$, we only need to show that $(\hat{v}^1, \hat{v}^2, \cdots, \hat{v}^n)$ is a spanning set of $V^*$ or linearly independent.
We have to check whether the equation $\sum_{i=1}^n x_i \hat{v}^i = 0$ for some $x_i \in \F$ has only the trivial solution.
Applying it to $\vec{v}_j$ for all $1 \leq j \leq n$, we have $0 = \sum_{i=1}^n x_i \hat{v}^i(\vec{v}_j) = \sum_{i=1}^n x_i \delta_j^i = x_j$.
So $x_j = 0$ for all $1 \leq j \leq n$.
This means that $(\hat{v}^1, \hat{v}^2, \cdots, \hat{v}^n)$ is linearly independent, and hence it is a basis of $V^*$.
We call it the \emph{dual basis} of the basis $\vec{v} = (v_1, v_2, \cdots, v_n)$ and denote it by $\vec{v}^* = (\hat{v}^1, \hat{v}^2, \cdots, \hat{v}^n)$.

Then we have to show that there is a unique basis in $V^*$ that satisfies $\hat{v}^i(\vec{v}_j) = \delta^i_j$.
Let $V = \F^n$ and $v = (\vec{v}_1, \vec{v}_2, \cdots, \vec{v}_n)$ be a basis of $V$.
Then $A = [\vec{v}_1 \quad \vec{v}_2 \quad \cdots \quad \vec{v}_n]$ is an invertible matrix.
Let $(\alpha^1, \alpha^2, \cdots, \alpha^n)$ be a basis of $V^*$.
Then we have the following equations:
\[
	[ \delta^i_j ] = \begin{bmatrix}
		\hdash & \alpha^1 & \hdash \\
		& \vdots \\
		\hdash & \alpha^n & \hdash
	\end{bmatrix} \begin{bmatrix}
		| & & | \\
		\vec{v_1} & \cdots & \vec{v_n} \\
		| & & |
	\end{bmatrix} = I_n
\]
Then $(\alpha_1, \alpha_2, \cdots, \alpha_n) = A^{-1}$.
So the dual basis is unique.

Finally, we have the natural identification:
\end{proof}
\begin{remark}
	$V \cong V^*$ but $\B_V \equiv \B_{V^*}$.
	The isomorphism $V \cong V^*$ depends on the choice of a basis in $\B_V$, while the natural isomorphism $\B_V \equiv \B_{V^*}$ does not depend on any choice.
\end{remark}

\begin{example}
	Consider the following open subset $U$ of $\R^2$:
	\begin{center}
		\begin{tikzpicture}
			\draw[gray,-Stealth,thin] (-1,0) -- (5,0) node[right] {$x$};
			\draw[gray,-Stealth,thin] (0,-1) -- (0,4) node[above] {$y$};

			\filldraw (3.5,1) circle (1pt) node[below right] {$p$};

			\draw[dashed] plot[smooth cycle, tension=1] coordinates{(4.5,1.5) (2.5,2.91) (0.5,1.5) (1.25,0.6) (2.5,1) (3.75,0.4)};
			\path (2.5,1.75) node {$U$};

			\draw[-Stealth, ocre] (3.5,1) -- (4,2) node[midway, right] {$\vec{u}$};
			\draw[-Stealth, ocre] (0,0) -- (0.5,1) node[midway, right] {$\vec{u}$};
		\end{tikzpicture}
	\end{center}
	Consider the cotangent vector $df_p$ at point $p$ for some smooth function $f : U \to \R$.
	It is a linear functional d$f_p : \text{T}_p U \to \R$ defined by d$f_p(\vec{u}) = \nabla f(p) \cdot \vec{u}$ for all $\vec{u} \in \text{T}_p U$.
	Here T$_p U$ is the tangent space of $U$ at point $p$, which is a vector space over $\R$.
	Note that both $\vec{u}$ and $\nabla f(p)$ are depending on the choice of a coordinate system.
	However, d$f_p$ is independent of any choice of coordinate system.
	In normal calculus, d$f_p$ is called the \emph{first partial derivative} of $f$ at point $p$, and normally we write it as $\frac{\partial f}{\partial x}(p)$ and $\frac{\partial f}{\partial y}(p)$.
\end{example}

The dual functor is not naturally isomorphic to the identity functor on $\Vect_\F$, as $(-)^*$ is a contravariant functor, while the identity is a contravariant functor, so there is no natural transformation from $\id_{\Vect_\F}$ to $(-)^*$.

\begin{center}
	\begin{tikzcd}[row sep=normal]
		\Vect_\F \arrow[r, "\id_{\Vect_\F}"] & \Vect_\F \\[-1.8em]
		Y \arrow[dd, "f" swap] & Y \arrow[dd, "f"] \\
		\arrow[r, mapsto, shorten <= 2ex, shorten >= 2ex] & \phantom{*} \\
		Z & Z
	\end{tikzcd}
	\qquad\qquad
	\begin{tikzcd}[row sep=normal]
		\Vect_\F \arrow[r, "(-)^*"] & \Vect_\F \\[-1.8em]
		Y \arrow[dd, "f" swap] & Y^* \\
		\arrow[r, mapsto, shorten <= 2ex, shorten >= 2ex] & \phantom{*} \\
		Z & Z^* \arrow[uu, "f^*" swap]
	\end{tikzcd}
\end{center}

\newpage

\section{Double Dual Spaces and Doubles}

Consider the endofunctors on $\Vect_\F$:
\begin{center}
	\begin{tikzcd}
		\Vect_\F \arrow[r, "(-)^{**}", yshift=0.5ex] \arrow[r, "\id_{\Vect_\F}" swap, yshift=-0.5ex] & \Vect_\F
	\end{tikzcd}
\end{center}
There is a natural transformation from $\id_{\Vect_\F}$ to $(-)^{**}$ defined by the natural identification: $V \equiv V^{**}$.
As $\id_{\Vect_\F}$ and $(-)^{**}$ are covariant functors, there is a natural transformation between them.

\begin{center}
	\begin{tikzcd}[row sep=normal]
		\Vect_\F \arrow[r, "\id_{\Vect_\F}"] & \Vect_\F \\[-1.8em]
		Y \arrow[dd, "f" swap] & Y \arrow[dd, "f"] \\
		\arrow[r, mapsto, shorten <= 2ex, shorten >= 2ex] & \phantom{*} \\
		Z & Z
	\end{tikzcd}
	\qquad\qquad
	\begin{tikzcd}[row sep=normal]
		\Vect_\F \arrow[r, "(-)^{**}"] & \Vect_\F \\[-1.8em]
		Y \arrow[dd, "f" swap] & Y^{**} \arrow[dd, "f^{**}" swap] \\
		\arrow[r, mapsto, shorten <= 2ex, shorten >= 2ex] & \phantom{*} \\
		Z & Z^{**}
	\end{tikzcd}
\end{center}

Let $\langle-,-\rangle : V^* \times V \to \F$ be the natural pairing defined by $\langle\alpha, u\rangle = \alpha(u)$ where $\alpha : V \to \F$ that sends $u \to \alpha{u}$.
It is the pairing of a covector with a vector and the map is bilinear.

\begin{definition}[Bilinear Maps]
	A map $B : U \times V \to W$ is called \emph{bilinear}\index{bilinear} if for all $u \in U$, the map $B(u, -) : V \to W$ is linear, and for all $v \in V$, the map $B(-, v) : U \to W$ is linear.
\end{definition}

We have the following natural identification:
\begin{center}
	\begin{tikzcd}[row sep=normal, column sep=normal]
		V^* \times V \arrow[rr, "{\langle-,-\rangle}"] & \arrow[d, "\vequiv" description, phantom] & \F \arrow[r, "\equiv" description, phantom]
		& V^* \arrow[rr, "1_{V^*}"] & \arrow[d, "\vequiv" description, phantom] & \Hom_\F(V, \F) \\


		V \times V^* \arrow[d, hook, two heads] \arrow[rr] & \phantom{*} & \F \arrow[r, "\equiv" description, phantom]
		& V \arrow[rr, "\iota_V"] & \phantom{*} & \Hom_\F(V^*, \F) \\[1.8em]


		V^* \times V \arrow[urr, "{\langle-,-\rangle}" swap]
	\end{tikzcd}
\end{center}
where $\iota_V : V \to V^{**}$ is defined by $\iota_V(u) = \check{u}$ such that $\check{u}(\alpha) = \alpha(u)$.
Then $V^{**} = \Hom_\F(V^*, \F) \equiv V$.

\begin{definition}[Doubles]
	Let $V$ be a linear space over $\F$.
	The \emph{double}\index{double} of $V$, denoted by $D(V)$, is defined as follows:
	\[
		D(V) = V \oplus V^*
	\]
\end{definition}

As $V$ is naturally isomorphic to $V^{**}$, we have the following natural identification:
\[
	D(V) = V \oplus V^* \equiv V^* \oplus V^{**} = D(V^*)
\]

The matrix representation of the isomorphism between $D(V)$ and $D(V^*)$ is
\[
	\begin{bmatrix}
		0 & -\iota_V \\
		1 & 0
	\end{bmatrix}
\]
where $\iota_V : V \to V^{**}$ is the natural isomorphism defined above.
The negative sign is used to make the isomorphism a symplectic isomorphism, which will be discussed in the later chapters.

\newpage

\section{Natural Transformation and Natural Equivalences}

\begin{definition}[Natural Transformations]
	Let $F, G : \C \to \D$ be two functors.
	A \emph{natural transformation} $\eta : F \to G$ is a collection of morphisms $\eta_X : F(X) \to G(X)$ in $\D$ for all objects $X$ in $\C$, such that for all morphisms $f : X \to Y$ in $\C$, the following diagram commutes:
	\begin{center}
		\begin{tikzcd}
			F(X) \arrow[r, "F(f)"] \arrow[d, "\eta_X" swap] & F(Y) \arrow[d, "\eta_Y"] \\
			G(X) \arrow[r, "G(f)"] & G(Y)
		\end{tikzcd}
	\end{center}
\end{definition}

\begin{definition}[Natural Equivalences]
	A \emph{natural equivalence} from functor $F$ to functor $G$ is a natural transformation $\eta : F \to G$ which has a two-sided inverse natural transformation $\eta^{-1} : G \to F$ such that $\eta \eta^{-1} = 1_G$ and $\eta^{-1} \eta = 1_F$.
	In this case, we say that $F$ and $G$ are \emph{naturally equivalent}, denoted by $F \equiv G$.
\end{definition}

\begin{example}
	Consider the endofunctors on $\Vect_\F$:
	\begin{center}
		\begin{tikzcd}
			\Vect_\F \arrow[r, "(-)^{**}", yshift=0.5ex] \arrow[r, "\id_{\Vect_\F}" swap, yshift=-0.5ex] & \Vect_\F
		\end{tikzcd}
	\end{center}
	We have the following natural transformation:
	\begin{center}
		\begin{tikzcd}
			(-)^{**} \arrow[d, "\vequiv", phantom] & V_1 \arrow[r, "f"] & V_2 \arrow[r, mapsto] & V_1^{**} \arrow[r, "f^{**}"] \arrow[d, "\eta_{V_1}" swap, ocre, "\cong"] & V_2^{**} \arrow[d, "\eta_{V_2}", ocre, "\cong" swap] \\
			\id_{\Vect_\F} & V_1 \arrow[r, "f"] & V_2 \arrow[r, mapsto] & V_1 \arrow[r, "f"] & V_2
		\end{tikzcd}
	\end{center}
	Then we have the natural equivalence: $(-)^{**} \equiv \id_{\Vect_\F}$.
\end{example}

\begin{example}
	We have the following natural equivalence:
	\[
		\Map^\BL(U \times V, -) \equiv \Hom_\F(U, \Hom_\F(V, -))
	\]
	where both are endofunctors on $\Vect_\F$.
	For any linear space $Z$ over $\F$, we have the natural isomorphism:
	\[
		{\musNatural}_Z : \Map^\BL(U \times V, Z) \to \Hom_\F(U, \Hom_\F(V, Z))
	\]
\end{example}

\begin{example}
	We have the following natural equivalence:
	\[
		\F \otimes - \equiv \id_{\Vect_\F} \equiv - \otimes \F \equiv \Hom_\F(\F, -) \equiv (-)^{**}
	\]
\end{example}

\newpage

\section{Exact Functors}

\begin{definition}[Covariant Exact Functors]
	Let $\C$ and $\D$ be two abelian categories.
	A covariant functor $F : \C \to \D$ is called:
	\begin{itemize}
		\item \emph{left exact} if whenever $0 \to A \to B \to C \to 0$ is exact then $0 \to F(A) \to F(B) \to F(C)$ is exact in $\D$, i.e., it preserves all finite limits;
		\item \emph{right exact} if whenever $0 \to A \to B \to C \to 0$ is exact then $F(A) \to F(B) \to F(C) \to 0$ is exact in $\D$, i.e., it preserves all finite colimits;
		\item \emph{exact}\index{exact} if it is both left exact and right exact.
	\end{itemize}
\end{definition}

\begin{definition}[Contravariant Exact Functors]
	Let $\C$ and $\D$ be two abelian categories.
	A contravariant functor $G : \C \to \D$, it is called:
	\begin{itemize}
		\item \emph{contravariant left exact} if whenever $0 \to A \to B \to C \to 0$ is exact then $0 \to G(C) \to G(B) \to G(A)$ is exact in $\D$;
		\item \emph{contravariant right exact} if whenever $0 \to A \to B \to C \to 0$ is exact then $G(C) \to G(B) \to G(A) \to 0$ is exact in $\D$;
		\item \emph{contravariant exact} if it is both contravariant left exact and contravariant right exact.
	\end{itemize}
\end{definition}

\begin{example}
	The dual functor $(-)^* : \Vect_\F \to \Vect_\F$ is a contravariant left exact functor, as it sends a short exact sequence $0 \to U \to V \to W \to 0$ to a left exact sequence $0 \to W^* \to V^* \to U^*$.
	Moreover, $U \to V \to W$ is exact if and only if $W^* \to V^* \to U^*$ is exact.
	Also, the map $U \to V$ is injective if and only if the map $V^* \to U^*$ is surjective; the map $U \to V$ is surjective if and only if the map $V^* \to U^*$ is injective.
	This can be shown by considering the following two exact sequences: $0 \to U \to V$ and $U \to V \to 0$.
\end{example}

In general, the hom-set functor $\Hom_\C(X, -) : \C \to \Set$ is a covariant left exact functor for any object $X$ in an abelian category $\C$, and the hom-set functor $\Hom_\C(-, X) : \C \to \Set$ is a contravariant left exact functor for any object $X$ in an abelian category $\C$.

\begin{example}
	The tensor product functor $- \otimes V$ is a covariant right exact functor, as it sends a short exact sequence $0 \to U \to V \to W \to 0$ to a right exact sequence $U \otimes V \to V \otimes V \to W \otimes V \to 0$.
\end{example}

Note that the tensor product functor is a left adjoint functor, and left adjoint functors are right exact in general, while the $\Vect_\F(V, -)$ functor is a right adjoint functor, and right adjoint functors are left exact in general.




\chapter{Tensor Algebra}

\epigraph{In high level universities, students will blame themselves if they don't understand the content, but in low level universities, students will blame the professors.}{Guowu Meng}

\section{Tensor Products}

Let $U$ and $V$ be two fixed linear spaces over $\F$ and $Z$ be any linear space over $\F$.
Consider the set of all bilinear maps from $U \times V$ to $Z$, denoted by $\Map^\BL(U \times V, Z)$.
It is a vector space over $\F$ as it is a subset of $\Map(U \times V, Z)$, the set of all maps from $U \times V$ to $Z$.

By the universal property of tensor product, we have a natural identification:
\[
	\Map^\BL(U \times V, Z) \equiv \Hom_\F(U \otimes V, Z)
\]
Note that both are naturally identical to $\Hom_\F(U, \Hom_\F(V, Z))$.
Also note that $\Hom(- \otimes V, Z) \equiv \Hom(-, \Hom(V, Z))$ is a \emph{tensor-hom adjunction}.

The natural identification is the universal property of tensor product.
Consider the following commutative diagram:
\begin{center}
	\begin{tikzcd}
		U \times V \arrow[r, "\forall \phi"] \arrow[d, "\iota" swap, hook] & Z \\
		U \otimes V \arrow[ur, dashed, "\exists ! \bar{\phi}" swap]
	\end{tikzcd}
\end{center}
Note that the map $\iota$ and $\phi$ are bilinear maps, and the existence of the unique linear map $\bar{\phi}$ follows from the universal property of the tensor product.
We can also consider it as the initial object in a new category:
\begin{itemize}
	\item Objects: all bilinear maps $\phi : U \times V \to Z$ for all $Z \in \Ob(\Vect_{\F)}$;
	\item Morphisms: commutative diagrams in $\Vect_\F$:
\end{itemize}
\begin{center}
	\begin{tikzcd}[row sep=normal]
		& Z \arrow[dd, "f"] \\
		U \times V \arrow[ur, "\phi"] \arrow[dr, "\phi'" swap] & \\
		& Z'
	\end{tikzcd}
\end{center}

The existence of tensor product follows from the existence of free vector space over a set and the existence of quotient spaces.

Consider the following commutative diagram:
\begin{center}
	\begin{tikzcd}
		& U \times V \arrow[dr, "\forall \phi"] \arrow[d, "\iota'" swap, hook] \\
		\ideal_{U, V} \arrow[r, hook] & \F[U \times V] \arrow[r, dashed, "\exists ! \phi'"] \arrow[d, "\pi" swap, two heads] & Z \\
		& \quotient{\F[U \times V]}{\ideal_{U, V}} \arrow[ur, dashed, "\exists ! \bar{\phi}" swap] \arrow[from=uu, bend right=60, "\iota" near start, swap, crossing over]
	\end{tikzcd}
\end{center}
where $\ideal_{U, V}$ is the subspace of $\F[U \times V]$ generated by the following elements for all $u, u_1, u_2 \in U$, $v, v_1, v_2 \in V$ and $\alpha, \beta \in \F$:
\begin{itemize}
	\item $(\alpha u_1 + \beta u_2, v) - \alpha (u_1, v) - \beta (u_2, v)$;
	\item $(u, \alpha v_1 + \beta v_2) - \alpha (u, v_1) - \beta (u, v_2)$;
\end{itemize}
Why the construction of $\ideal_{U, V}$ is like this?
This is because we want $\iota$ to be a bilinear map.
Then $\iota(\alpha u_1 + \beta u_2, v) = \alpha \iota(u_1, v) + \beta \iota(u_2, v)$ and $\iota(u, \alpha v_1 + \beta v_2) = \alpha \iota(u, v_1) + \beta \iota(u, v_2)$.
This means that the elements in $\ideal_{U, V}$ should be mapped to $0$ by $\iota$.
So we have to quotient $\F[U \times V]$ by $\ideal_{U, V}$ to make $\iota$ a bilinear map.

We define $U \otimes V = \quotient{\F[U \times V]}{\ideal_{U, V}}$ and this shows the existence of tensor product.
\begin{remark}
	The inclusion map $\iota : U \times V \to U \otimes V$ is `surjective' in the sense that the image of $\iota$ spans $U \otimes V$, i.e.~$\Span(\im{\iota)} = U \otimes V$.
	To know $\bar{\phi}$, it suffices to know $\bar{\phi}(u \otimes v) = \phi(u, v)$ for all $u \in U$ and $v \in V$.
\end{remark}

We can talk about the tensor product of $k$ linear spaces with $k \geq 2$.
Moreover, the tensor product is associative and commutative up to isomorphism, i.e., $V_1 \otimes V_2 \otimes V_3 \cong (V_1 \otimes V_2) \otimes V_3 \cong V_1 \otimes (V_2 \otimes V_3)$ and $V_1 \otimes V_2 \cong V_2 \otimes V_1$.
Both of them are natural isomorphisms.

\begin{center}
	\begin{tikzcd}[row sep=normal]
		V_1 \times V_2 \times V_3 \arrow[r] \arrow[d] & V_1 \otimes V_2 \otimes V_3 \\
		(V_1 \otimes V_2) \times V_3 \arrow[r] & (V_1 \otimes V_2) \otimes V_3 \arrow[u, "\vequiv" description, phantom]
	\end{tikzcd}
	\qquad
	\begin{tikzcd}[row sep=normal]
		V_1 \times V_2 \arrow[r] \arrow[d, leftrightarrow] & V_1 \otimes V_2 \\
		V_2 \times V_1 \arrow[r] & V_2 \otimes V_1 \arrow[u, "\vequiv" description, phantom]
	\end{tikzcd}
\end{center}

We have a natural equivalence:
\[
	\Hom(U, V \otimes W) \equiv \Hom(U, V) \otimes W
\]
Then we can prove that $\Hom(V_1, V_2) \equiv V_1^* \otimes V_2$ and $(V_1 \otimes V_2)^* \equiv V_1^* \otimes V_2^*$.
Also, we have the following equation, by considering $V_1 \otimes V_2 \equiv \Hom(V_1^*, V_2)$:
\[
	\dim(V \otimes W) = \dim(V) \cdot \dim(W)
\]
If $e$ is a minimal spanning set of $V_1$ and $f$ is a minimal spanning set of $V_2$, then $e \otimes f$ is a minimal spanning set of $V_1 \otimes V_2$.
Moreover, we have $\End(V) \equiv (\End(V))^*$ and the identity map $1_V$ corresponds to the \emph{trace}\index{trace} map $\tr : \End(V) \to \F$ under this identification.

We also have the distribution of tensor product over direct sum: $V_1 \otimes (V_2 \oplus V_3) \equiv (V_1 \otimes V_2) \oplus (V_1 \otimes V_3)$.
Moreover, $\Hom(V_1, V_2 \oplus V_3) \equiv \Hom(V_1, V_2) \oplus \Hom(V_1, V_3)$ and $\Hom(V_1 \oplus V_2, V_3) \equiv \Hom(V_1, V_3) \times \Hom(V_2, V_3)$.

\newpage

\section{Algebras}

\begin{definition}[Algebras]
	An \emph{algebra}\index{algebra} over a field $\F$ is a linear space $A$ over $\F$ equipped with a bilinear product map $A \times A \to A$, or equivalently a linear map $A \otimes A \to A$.
\end{definition}

\begin{example}
	The set of all polynomials in $t$ with coefficients in $\F$, denoted $\F[t]$, is an algebra over $\F$.
	As $\F[t] \times \F[t] \to \F[t]$ defined by $(f, g) \mapsto fg$ is a bilinear map.
	Moreover, $\F[t]$ has a multiplicative identity $1 \in \F[t]$, $fg = gf$ for all $f, g \in \F[t]$, and $(fg)h = f(gh)$ for all $f, g, h \in \F[t]$.
	So $\F[t]$ is a unital commutative associative algebra over $\F$.
\end{example}

\begin{example}
	The set of all square matrices with order $n$ over $\F$, denoted by $\M{n \times n}{\F}$, is an algebra over $\F$.
	As $\M{n \times n}{\F} \times \M{n \times n}{\F} \to \M{n \times n}{\F}$ defined by $(A, B) \mapsto AB$ is a bilinear map.
	Moreover, $\M{n \times n}{\F}$ has a multiplicative identity $I_n \in \M{n \times n}{\F}$, $(AB)C = A(BC)$ for all $A, B, C \in \M{n \times n}{\F}$.
	However, in general $AB \neq BA$ for some $A, B \in \M{n \times n}{\F}$.
	So $\M{n \times n}{\F}$ is a unital associative algebra but it is a non-commutative algebra over $\F$.
\end{example}

\begin{example}
	The 3-dimensional Euclidean space $\R^3$ with the cross product $\times : \R^3 \times \R^3 \to \R^3$ is an algebra over $\R$.
	As the cross product is bilinear.
	However, it does not have a multiplicative identity, not associative and not commutative.
	So $\R^3$ with the cross product is a non-unital non-associative non-commutative algebra over $\R$.
\end{example}
\begin{remark}
	$(\R^3, \times)$ is an example of a simple real lie algebra.
	It is the lie algebra of the lie group SO(3), the special orthogonal group in dimension 3, i.e., the 3-dimensional rotations. $(\R^3, \times)$ is denoted by $\mathfrak{so}(3)$.
	Also, it is the lie algebra of the infinitesimal symmetries of a pointed 3-dimensional Euclidean space.
\end{remark}

\begin{definition}[Lie Algebras]
	An algebra is $\mathfrak{g}$ over a field $\F$ is called a \emph{lie algebra} if the \emph{lie bracket} or lie product $[-, -] : \mathfrak{g} \times \mathfrak{g} \to \mathfrak{g}$ satisfies the following two conditions:
	\begin{itemize}
		\item Skew-symmetry: $[x, x] = 0$ for all $x \in \mathfrak{g}$, i.e., $[x, y] = -[y, x]$ for all $x, y \in \mathfrak{g}$ if $\chart(\F) \neq 2$;
		\item Jacobi Identity: $[x, [y, z]] + [y, [z, x]] + [z, [x, y]] = 0$ for all $x, y, z \in \mathfrak{g}$.
	\end{itemize}
\end{definition}

\begin{definition}[Graded Linear Space]
	A linear space $V_\smallbullet$ over $\F$ is called a \emph{$\Z_{\geq 0}$-graded linear space} or \emph{graded vector space} if it is a direct sum of linear subspaces $V_n$ for all $n \in \Z_{\geq 0}$:
	\[
		V_\smallbullet = \bigoplus_{n = 0}^\infty V_n
	\]
	The elements in $V_n$ are called \emph{homogeneous elements} of degree $n$.
	If $v \in V_n$ is a homogeneous element, we write $\deg(v) = n$.
\end{definition}

\begin{definition}[Graded Linear Maps]
	A linear map $\phi : V_\smallbullet \to W_\smallbullet$ is called a \emph{graded linear map} with graded degree $k \geq 0$ if $\phi(V_n) \subseteq W_{n + k}$ for all $n \in \Z_{\geq 0}$.
\end{definition}

\newpage

\section{Tensor Algebras}

Let $V$ be a finite dimensional linear space over $\F$.
We define a new notation:
\[
	V^{\otimes k} = \underbrace{V \otimes V \otimes \cdots \otimes V}_{k \text{ times}}
\]
for all $k \geq 0$.
Note that $V^{\otimes 0} = \F$.
Also, $\dim(V^{\otimes k}) = (\dim{V})^k$ for all $k \geq 0$.

We define the \emph{tensor algebra} of $V$ over $\F$, denoted by $\T V$, as follows:
\[
	\T V = \bigoplus_{k = 0}^\infty V^{\otimes k} = \F \oplus V \oplus (V \otimes V) \oplus (V \otimes V \otimes V) \oplus \cdots
\]

The tensor algebra $\T V$ is an algebra over $\F$ with the bilinear product map defined by the tensor product:
\[
	\otimes : \T V \times \T V \to \T V
\]
which sends $(\sum_n u_n, \sum_m v_m)$ to $\sum_{n, m} (u_n \otimes v_m)$.
\begin{remark}
	As the algebra product is bilinear, it suffices to know the product of two homogeneous elements, i.e., $V^{\otimes n} \times V^{\otimes m} \to \T V$ for all $n, m \geq 0$.
	So $\T V$ is a $\Z_{\geq 0}$-graded algebra over $\F$.
	As the tensor algebra is bi-additive, we have the following equality:
	\[
		\sum_n u_n \otimes \sum_m v_m = \sum_n (u_n \otimes \sum_m v_m) = \sum_n \sum_m (u_n \otimes v_m) = \sum_{n, m} (u_n \otimes v_m)
	\]
\end{remark}

Then to define the bilinear product above, we have to define the tensor product of two homogeneous elements:
\begin{center}
	\begin{tikzcd}[row sep=normal, column sep=normal]
		V^{\otimes n} \times V^{\otimes m} \arrow[rr] \arrow[dd] & & \T V \\
		& V^{\otimes (n + m)} \arrow[ur, hook] & \\
		V^{\otimes n} \otimes V^{\otimes m} \arrow[ur, dashed]
	\end{tikzcd}
\end{center}

We have to prove the existence of the bilinear map $V^{\otimes n} \times V^{\otimes m} \to V^{\otimes (n + m)}$ for all $n, m \geq 0$.
We can prove it by the following commutative diagram:
\begin{center}
	\begin{tikzcd}[row sep=normal, column sep=normal]
		V^{\otimes n} \times V^{\otimes m} \arrow[r] & \overbrace{V \otimes \cdots \otimes V}^{n \text{ times}} \otimes \overbrace{V \otimes \cdots \otimes V}^{m \text{ times}} \\
		\underbrace{(V \times \cdots \times V)}_{n \text{ times}} \times \underbrace{(V \times \cdots \times V)}_{m \text{ times}} \arrow[u] & \\
		\underbrace{V \times \cdots \times V}_{n + m \text{ times}} \arrow[u] \arrow[uur, "\phi" swap] \arrow[r] & \underbrace{V \otimes \cdots \otimes V}_{n + m \text{ times}} \arrow[uu, "\exists ! \bar{\phi}" swap, dashed]
	\end{tikzcd}
\end{center}
The proof used a lot of universal properties of tensor products.
Note that the map $\phi$ is a multilinear map and $\bar{\phi}$ is a linear equivalence.

So we have proved the existence of the bilinear product map $\otimes : \T V \times \T V \to \T V$.
Then $\T V$ is an algebra over $\F$.
\begin{remark}
	The tensor algebra $(\T V, \otimes)$ is a graded unital associative algebra over $\F$.
	It is graded, as it is degree additive, i.e., $V^{\otimes n} \times V^{\otimes m} \to V^{\otimes (n + m)}$ for all $n, m \geq 0$.
	It is unital, as $V^{\otimes 0} - \F \times V^{\otimes m} \to V^{\otimes m}$ and the reverse.
	The multiplicative identity is $1 \in \F$.
	It is associative, as $(u \otimes v) \otimes w = u \otimes (v \otimes w)$ for all $u, v, w \in V$ and the associativity can be extended to all homogeneous elements by bi-additivity.
	However, in general it is not commutative, as $u \otimes v \neq v \otimes u$ for some $u, v \in V$.
\end{remark}

There is a universal property of tensor algebras.
Consider the following commutative diagram:
\begin{center}
	\begin{tikzcd}
		V \arrow[r, "\forall \phi"] \arrow[d, "\iota" swap, hook] & A^\smallbullet \\
		\T V \arrow[ur, dashed, "\exists ! \bar{\phi}" swap]
	\end{tikzcd}
\end{center}
Note that $V = V^{\otimes 1} = 0 \oplus V \oplus 0 \oplus \cdots \subseteq \T V$ and $\iota$ is the inclusion map.
Here $A^\smallbullet$ is any graded unital associative algebra over $\F$ and $\phi : V \to A^\smallbullet$ is a graded linear map with graded degree $0$.
Then there exists a unique graded algebra homomorphism with graded degree $0$ $\bar{\phi} : \T V \to A^\smallbullet$ such that $\bar{\phi} \circ \iota = \phi$.
This shows the universal property of tensor algebras.
More specifically, the map $\phi$ is a map from $V$ to the degree $1$ part of $A^\smallbullet$, i.e., $\phi : V \to A_1$, then with an inclusion map.

The tensor algebra construction is actually a functor from $\Vect_\F$ to the category of graded unital associative algebras over $\F$, denoted by $\Z_{\geq 0} - \Alg_\F$:
\begin{center}
	\begin{tikzcd}[row sep=normal]
		\Vect_\F \arrow[r, "\T"] & \Z_{\geq 0} - \Alg_\F \\[-1.8em]
		V \arrow[dd, "f" swap] & \T V \arrow[dd, "\T f"] \\
		\arrow[r, mapsto, shorten <= 2ex, shorten >= 2ex] & \phantom{*} \\
		W & \T W
	\end{tikzcd}
\end{center}
where $\T f : \T V \to \T W$ is the unique graded algebra homomorphism with graded degree $0$ such that $\T f \circ \iota_V = \iota_W \circ f$.
Here $\iota_V : V \to \T V$ and $\iota_W : W \to \T W$ are the inclusion maps.

The existence of the functor $\T$ follows from the universal property of tensor algebras.
It is called the \emph{free graded algebra functor}, normally the ``unital'' and ``associative'' will be omitted.

\newpage

\section{Quotient Algebras}

In this section, we will discuss three quotient algebras associated with a vector space $V$: the symmetric algebra, exterior algebra, and universal enveloping algebra.

Before that we need to introduce the concept of ideals in algebras.

\begin{definition}[Ideals of Algebras]
	An \emph{ideal}\index{ideal} of an algebra $A$ over a field $\F$ is a non-empty subset $I$ of $A$ which is closed under linear combinations and algebra multiplications by elements in $A$.
	That is, for all $x, y \in I$, $\alpha, \beta \in \F$ and $a \in A$, we have:
	\begin{itemize}
		\item $\alpha x + \beta y \in I$;
		\item $ax \in I$ and $xa \in I$.
	\end{itemize}
\end{definition}

Simply speaking, an ideal is a generalisation of a rule to an algebra.

The following is an example of an ideal in a ring, which is an example of ideal in a more general concept.
\begin{example}
	Consider the ring of integers, $\Z$.
	The set of all $n$-multiples, denoted by $n\Z$, is an ideal of $\Z$ for all $n \in \Z$.
	As it is closed under addition and multiplication by any integer.
\end{example}

\subsection{Symmetric Algebras}

The \emph{symmetric algebra} of a vector space $V$ over a field $\F$, denoted by $\Sym V$, is defined as the quotient algebra of the tensor algebra $\T V$ by the ideal of $\T V$ generated by elements of the form $u \otimes v - v \otimes u$ for all $u, v \in V$:
\[
	\Sym V = \quotient{\T V}{\ideal_\Sym} = \quotient{\T V}{\langle u \otimes v - v \otimes u \mid u, v \in V \rangle}
\]
The $\ideal_\Sym$ is called the \emph{symmetrising ideal} of $\T V$.
It is actually the \emph{ideal completion} of the relation $u \otimes v = v \otimes u$ for all $u, v \in V$.
We use $\langle - \rangle$ to denote the ideal generated by a set.

Then the elements in $\Sym V$ are equivalence classes of elements in $\T V$.
We have $uv \in \Sym V$ as the equivalence class of $u \otimes v \in \T V$ denoted by $[u \otimes v]$.
Note that $uv = [u \otimes v] = [v \otimes u] = vu$ in $\Sym V$, as $[u \otimes v - v \otimes u] = 0$.
So the product in $\Sym V$ is commutative.
\begin{remark}
	Symmetric algebra is still a graded algebra.
	As the ideal $\ideal_\Sym$ is a graded ideal, i.e., $\ideal_\Sym = \bigoplus_{k = 0}^\infty (\ideal_\Sym \cap V^{\otimes k})$.
\end{remark}

Similar to tensor algebras, we have the following expression:
\[
	\Sym V = \bigoplus_{k = 0}^\infty \mathcal{S}^k V
\]
where $\mathcal{S}^k V$ is the $k$-th symmetric power of $V$.

We also have the following universal property of symmetric algebras.
Consider the following commutative diagram:
\begin{center}
	\begin{tikzcd}[row sep = normal]
		V \arrow[r, "\forall \phi"] \arrow[d, "\iota" swap, hook] & A^\smallbullet \\
		\T V \arrow[d, "\pi" swap, two heads] \arrow[ur, dashed] & \\
		\Sym V \arrow[uur, dashed, "\exists ! \bar{\phi}" swap]
	\end{tikzcd}
\end{center}
Here $A^\smallbullet$ is any graded unital commutative associative algebra over $\F$.

Similarly, $\Sym V$ is the \emph{free graded commutative algebra functor} from $\Vect_\F$ to the category of graded unital commutative associative algebras over $\F$, denoted by $\Z_{\geq 0} - \CAlg_\F$:

\subsection{Exterior Algebras}

The \emph{exterior algebra} of a vector space $V$ over a field $\F$, denoted by $\Ext V$, is defined as the quotient algebra of the tensor algebra $\T V$ by the ideal of $\T V$ generated by elements of the form $v \otimes v$ for all $v \in V$:
\[
	\Ext V = \quotient{\T V}{\ideal_\Ext} = \quotient{\T V}{\langle v \otimes v \mid v \in V \rangle} = \quotient{\T V}{\langle v \otimes w + w \otimes v \mid v, w \in V \rangle}
\]
The $\ideal_\Ext$ is called the \emph{alternating ideal} of $\T V$.
It is actually the \emph{ideal completion} of the relation $v \otimes v = 0$ for all $v \in V$, or equivalently $v \otimes w = - w \otimes v$ for all $v, w \in V$.
Sometimes the exterior algebra is also called the \emph{skew-symmetric algebra}.
Note that the characteristic of the field $\F$ should not be $2$, i.e., $\chart(\F) \neq 2$, otherwise $v \otimes w = - w \otimes v$ implies that $v \otimes w = w \otimes v$.

Then the product in $\Ext V$ is called the \emph{exterior product} or \emph{wedge product}, denoted by $\wedge$.
We have $u \wedge v = - v \wedge u$ in $\Ext V$ for all $u, v \in V$.
So the product in $\Ext V$ is skew-commutative.
\begin{remark}
	Exterior algebra is still a graded algebra.
	As the ideal $\ideal_\Ext$ is a graded ideal, i.e., $\ideal_\Ext = \bigoplus_{k = 0}^\infty (\ideal_\Ext \cap V^{\otimes k})$.
\end{remark}

Then we have the following expression:
\[
	\Ext V = \bigoplus_{k = 0}^\infty {\bigwedge}^k V
\]
where ${\bigwedge}^k V$ is the $k$-th exterior power of $V$.

We also have the following universal property of exterior algebras.
Consider the following commutative diagram:
\begin{center}
	\begin{tikzcd}[row sep = normal]
		V \arrow[r, "\forall \phi"] \arrow[d, "\iota" swap, hook] & A^\smallbullet \\
		\T V \arrow[d, "\pi" swap, two heads] \arrow[ur, dashed] & \\
		\Ext V \arrow[uur, dashed, "\exists ! \bar{\phi}" swap]
	\end{tikzcd}
\end{center}
Here $A^\smallbullet$ is any graded unital associative skew-commutative algebra over $\F$.

Similarly, $\Ext V$ is the \emph{free graded skew-commutative algebra functor} from $\Vect_\F$ to the category of graded unital associative skew-commutative algebras over $\F$, denoted by $\Z_{\geq 0} - \SAlg_\F$:

\subsection{Universal Enveloping Algebras}

Let $\mathfrak{g}$ be a lie algebra over a field $\F$.
The \emph{universal enveloping algebra} of $\mathfrak{g}$ over $\F$, denoted by $\Env \mathfrak{g}$, is defined as the quotient algebra of the tensor algebra $\T \mathfrak{g}$ by the ideal of $\T \mathfrak{g}$ generated by elements of the form $x \otimes y - y \otimes x - [x, y]$ for all $x, y \in \mathfrak{g}$:
\[
	\Env \mathfrak{g} = \quotient{\T \mathfrak{g}}{\ideal_\Env} = \quotient{\T \mathfrak{g}}{\langle x \otimes y - y \otimes x - [x, y] \mid x, y \in \mathfrak{g} \rangle}
\]
The $\ideal_\Env$ is called the \emph{lie ideal} of $\T \mathfrak{g}$.
It is actually the \emph{ideal completion} of the relation $xy - yx = [x, y]$ for all $x, y \in \mathfrak{g}$.
\begin{remark}
	However, the universal enveloping algebra is not a graded algebra.
	As the ideal $\ideal_\Env$ is not a graded ideal.
	The $x \otimes y - y \otimes x$ is in $\mathfrak{g}^{\otimes 2}$ but $[x, y]$ is in $\mathfrak{g}^{\otimes 1}$.
\end{remark}

\newpage

\section{Hilbert-Poincar Series}
Let $V_\smallbullet = \bigoplus_{i \geq 0} V_i$ be a $\Z_{\geq 0}$-graded finite dimensional linear space over a field $\F$.
The \emph{Hilbert-Poincar series} of $V_\smallbullet$ is defined as the following formal power series:
\[
	P_{V_\smallbullet}(t) = \sum_{i = 0}^\infty \dim(V_i)\ t^i
\]

\begin{example}
	The Hilbert-Poincar series of the tensor algebra $\T V$ is:
	\[
		P_{\T V}(t) = \sum_{i = 0}^\infty \dim(V^{\otimes i})\ t^i = \sum_{i = 0}^\infty (\dim{V})^i\ t^i = \frac{1}{1 - \dim{V}\ t}
	\]
\end{example}

\begin{example}
	The Hilbert-Poincar series of the symmetric algebra $\Sym V$ is:
	\[
		P_{\Sym V}(t) = \sum_{i = 0}^\infty \dim(\mathcal{S}^i V)\ t^i = \frac{1}{(1 - t)^{\dim{V}}}
	\]
\end{example}

\begin{example}
	The Hilbert-Poincar series of the exterior algebra $\Ext V$ is:
	\[
		P_{\Ext V}(t) = \sum_{i = 0}^\infty \dim({\bigwedge}^i V)\ t^i = (1 + t)^{\dim{V}}
	\]
\end{example}

As the Hilbert-Poincar series of the exterior algebra $\Ext V$ is a polynomial of degree $\dim{V}$, we have ${\bigwedge}^k V = 0$ for all $k > \dim{V}$.
Especially, if $\dim{V} = n$, then ${\bigwedge}^n V$ is 1-dimensional and ${\bigwedge}^{n + 1} V = 0$.
This is because any $(n + 1)$ vectors in an $n$-dimensional vector space are linearly dependent, so the exterior product of them is $0$.
Moreover, $\dim({\bigwedge}^k V) = \dim({\bigwedge}^{n - k} V)$ for all $0 \leq k \leq n$.

Any one-dimensional linear space is called a \emph{line}\index{line}.
\chapter{Determinants}

\epigraph{``If you are willing to prove it, you can prove it. There is no trick.''}{Guowu Meng}

\section{Determinant Lines}

We have known that the top exterior power ${\bigwedge}^n V$ of an $n$-dimensional vector space $V$ over a field $\F$ is 1-dimensional.
So we can define the following:

\begin{definition}[Determinant Lines]
	The \emph{determinant line} of an $n$-dimensional vector space $V$ over a field $\F$ is defined as the top exterior power of $V$:
	\[
		\det{V} = {\bigwedge}^n V = {\bigwedge}^{\dim{V}} V
	\]
\end{definition}

Note that the $\det = {\bigwedge}^k$ is a functor from the category of vector spaces with $n$-dimensions $\Vect_\F^n$ to the category of vector spaces with 1-dimensional, i.e., the category of lines, $\Vect_\F^1$ for all $k \geq 0$:
\begin{center}
	\begin{tikzcd}[row sep=normal]
		\Vect_\F^n \arrow[r, "{\bigwedge}^n = \det"] & \Vect_\F^1 \\[-1.8em]
		V_1 \arrow[dd, "f" swap] & {\bigwedge}^n V_1 = \det{V_1} \arrow[dd, "{\bigwedge}^n f = \det{f}"] \\
		\arrow[r, mapsto, shorten <= 2ex, shorten >= 2ex] & \phantom{*} \\
		V_2 & {\bigwedge}^n V_2 = \det{V_2}
	\end{tikzcd}
\end{center}

As $\det$ is a functor, we have the following two properties:
\[
	\det \id_V = \id_{\det V}, \qquad \det fg = \det f \cdot \det g
\]

In particular, if $f \in \End(V)$, then $\det f : \det V \to \det V$ is a multiplication by a scalar in $\F$.
So we can identify $\det f$ with a scalar in $\F$.
This scalar is called the \emph{determinant}\index{determinant} of $f$ and is denoted by $\det f$.

Consider the following commutative diagram:
\begin{center}
	\begin{tikzcd}
		\F^n \arrow[r, "A'", red] \arrow[dd, bend right=60, "P" swap, ocre] & \F^n \arrow[dd, bend left=60, "P", red] \\
		V \arrow[r, "f"] \arrow[u, "{[-]_{\B'}}"] \arrow[d, "{[-]_\B}" swap] & V \arrow[u, "{[-]_\B}" swap] \arrow[d, "{[-]_{\B'}}"] \\
		\F^n \arrow[r, "A", ocre] & \F^n
	\end{tikzcd}
\end{center}
where $V$ is an $n$-dimensional vector space over $\F$, $\B$ and $\B'$ are two bases of $V$, $f \in \End(V)$, $A$ and $A'$ are the matrix representations of $f$ under the bases $\B$ and $\B'$ respectively, and $P$ is the change of basis matrix from $\B$ to $\B'$.
Then by focusing the red and blue commutative square, we have:
\[
	AP = PA', \qquad A = P A' P^{-1}
\]

Then we have:
\[
	\det A \overset{\text{def}}{=\joinrel=} \det f \overset{\text{def}}{=\joinrel=} \det A'
\]

In ordinary linear algebra, $A$ and $A'$ are called \emph{similar matrices}, i.e., $A \sim A'$.
This means they represent the same endomorphism, so they have the same determinant.

\newpage

\section{Permutation Groups}

Before we derive the explicit formula of determinants, we need to introduce the concept of permutation groups.

\begin{definition}[Automorphisms]
	An \emph{automorphism}\index{automorphism} is an isomorphism from a mathematical object to itself.
\end{definition}

\begin{definition}[Automorphism Groups]
	The set of all automorphisms on a mathematical object $X$ forms a group under the composition of functions, denoted by $\Aut(X)$.
\end{definition}

\begin{example}
	The general linear group $\GL(V)$ of a vector space $V$ over $\F$ is the group of all invertible linear maps from $V$ to $V$, i.e., $\GL(V) = \Aut(V)$.
	The group operation is the composition of functions.
\end{example}

\begin{example}
	The general linear group $\GL_n(\F)$ of degree $n$ over $\F$ is the group of all invertible $n \times n$ matrices over $\F$, i.e., $\GL_n(\F) = \Aut(\F^n)$.
	The group operation is the matrix multiplication.
	Note that $\GL_n(\F) \cong \GL(\F^n)$.
	Also note that the group is not abelian if $n \geq 2$.
\end{example}

\begin{definition}[Permutation Groups]
	A \emph{permutation group} $S_n$ on a set $\underline{n} := \{ 1, 2, \cdots, n \}$ is the group of all bijections from $\underline{n}$ to itself, i.e., $S_n = \Aut(\underline{n})$.
	It is called the \emph{symmetric group} on $n$ elements.
	The group operation is the composition of functions.
\end{definition}

Then the order of $S_n$, denoted by $|S_n|$, is $n!$.

\begin{example}
	The permutation group $S_2$ has two elements: the identity permutation $1$ and the transposition $\sigma_1$ defined by $\sigma_1(1) = 2$ and $\sigma_1(2) = 1$.
\end{example}

Instead of writing $S_2 = \{ 1, \sigma_1 \}$, we can write $S_2 = \langle \sigma_1 \mid \sigma_1^2 = 1 \rangle$, where $\sigma_1$ is called the \emph{generator}\index{generator} of $S_2$ and $\sigma_1^2 = 1$ is called the \emph{relation}\index{relation} of $S_2$.
This is called the \emph{presentation}\index{presentation} of $S_2$.

In general, the generator $\sigma_i$ of $S_n$ is defined by:
\[
	\sigma_i(j) = \begin{cases}
		j + 1, & j = i \\
		j - 1, & j = i + 1 \\
		j, & \text{otherwise}
	\end{cases} = (i \quad i + 1)
\]

\begin{example}
	The generator $\sigma_1$ of $S_3$ can be represented by the following diagram:
	\begin{center}
		\begin{tikzcd}
			1 \arrow[dr] & 2 \arrow[dl, crossing over] & 3 \arrow[d] \\
			2 & 1 & 3
		\end{tikzcd}
	\end{center}
	It can also be written as $\sigma_1 = (1 \quad 2)$ or $(1 \quad 2)(3)$ or $\begin{pmatrix}
		1 & 2 & 3 \\
		2 & 1 & 3
	\end{pmatrix}$.
\end{example}

Moreover, we have a cycle with 3 elements denoted as $(1 \quad 2 \quad 3)$ defined by the $\begin{pmatrix}
	1 & 2 & 3 \\
	2 & 3 & 1
\end{pmatrix}$.

Then the presentation of $S_3$ is:
\[
	S_3 = \langle \sigma_1, \sigma_2 \mid \sigma_1^2 = 1, \sigma_2^2 = 1, \sigma_1 \sigma_2 \sigma_1 = \sigma_2 \sigma_1 \sigma_2 \rangle
\]

In general, the presentation of $S_n$ is:
\[
	S_n = \langle \sigma_1, \sigma_2, \cdots, \sigma_{n - 1} \mid \sigma_i^2 = 1, \sigma_i \sigma_j = \sigma_j \sigma_i\ (|i - j| > 1), \sigma_i \sigma_{i + 1} \sigma_i = \sigma_{i + 1} \sigma_i \sigma_{i + 1} \rangle
\]

The last two relations are called the \emph{braid relations}:
\begin{itemize}
	\item \emph{Far commutativity:} $\sigma_i \sigma_j = \sigma_j \sigma_i$ for all $|i - j| > 1$;
	\item \emph{Braid relation:} $\sigma_i \sigma_{i + 1} \sigma_i = \sigma_{i + 1} \sigma_i \sigma_{i + 1}$.
\end{itemize}

The permutation group $S_n$ is generated by quotienting the braid group $B_n$ by the relations $\sigma_i^2 = 1$ for all $1 \leq i \leq n - 1$.
We call $B_n$ the \emph{braid group} on $n$ strands.
A simple way to visualise the braid group is to think about braiding $n$ strands of hair.
The braid group $B_n$ has the same presentation as $S_n$ except that there is no relation $\sigma_i^2 = 1$ for all $1 \leq i \leq n - 1$.
Consider the following diagrams:
\begin{center}
	\begin{tikzcd}[column sep=normal]
		1 \arrow[d, dash] & 2 \arrow[d, dash] \\
		1 & 2
	\end{tikzcd}
	\qquad
	$\xLongrightarrow{\sigma_1}$
	\qquad
	\begin{tikzcd}[column sep=normal]
		1 \arrow[dr, dash, start anchor=south, end anchor=north] & 2 \arrow[dl, crossing over, dash, start anchor=south, end anchor=north] \\
		2 & 1
	\end{tikzcd}
	\qquad
	$\xLongrightarrow{\sigma_1}$
	\qquad
	\begin{tikzcd}[column sep=normal, row sep=normal]
		1 \arrow[dr, dash, start anchor=south, end anchor=center] & 2 \arrow[dl, crossing over, dash, start anchor=south, end anchor=center] \\
		\phantom{2} \arrow[dr, dash, start anchor=center, end anchor=north] & \phantom{1} \arrow[dl, crossing over, dash, start anchor=center, end anchor=north, shorten=1cm] \arrow[dl, dash, start anchor=center, end anchor=north] \\
		1 & 2
	\end{tikzcd}
\end{center}

Consider the following exact sequence:
\begin{center}
	\begin{tikzcd}
		1 \arrow[r] & A_n \arrow[r, hook] & S_n \arrow[r, "\sgn", two heads] & \{ \pm 1 \} \arrow[r] & 1
	\end{tikzcd}
\end{center}
where $A_n$ is the \emph{alternating group} on $n$ elements, i.e., the subgroup of $S_n$ consisting of all even permutations, and $\sgn : S_n \to \{ \pm 1 \}$, the \emph{sign homomorphism}, is the unique group homomorphism such that $\sgn(\sigma_i) = -1$ for all $1 \leq i \leq n - 1$.
Note that $\ker(\sgn) = A_n$ and $\im{\sgn} = \{ \pm 1 \}$.
\begin{remark}
	$A_n$ is simple for all $n \geq 5$.
	This means that $A_n$ has no non-trivial normal subgroups for all $n \geq 5$.
\end{remark}

Then we have two properties of the sign homomorphism:
\begin{itemize}
	\item $\sgn(1) = 1$;
	\item $\sgn(\sigma \tau) = \sgn(\sigma) \cdot \sgn(\tau)$ for all $\sigma, \tau \in S_n$.
\end{itemize}

\newpage

\section{Universal Property of Exterior Powers}

We have known that the $k$-th exterior power ${\bigwedge}^k V$ of a vector space $V$ over a field $\F$ is the quotient of the $k$-th tensor power $V^{\otimes k}$ by the alternating ideal.
So, consider $\dim{V} = n$, we have the following commutative diagram:
\begin{center}
	\begin{tikzcd}[row sep = normal]
		\overbrace{V \times V \times \cdots \times V}^{n \text{ times}} \arrow[r, "\forall \phi"] \arrow[d, "\iota"', hook] & Z \\
		V^{\otimes n} \arrow[d, "\pi"', two heads] \arrow[ur, dashed] \\
		{\bigwedge}^n V \arrow[uur, dashed, "\exists ! \bar{\phi}" swap]
	\end{tikzcd}
\end{center}
Here $Z$ is any vector space over $\F$ and $\phi : V \times V \times \cdots \times V \to Z$ is an alternating (skew-symmetric) multilinear map, i.e., $\phi(v_1, v_2, \cdots, v_n) = 0$ if $v_i = v_j$ for some $i \neq j$.
Then there exists a unique linear map $\bar{\phi} : {\bigwedge}^n V \to Z$ such that $\bar{\phi} \circ \pi \circ \iota = \phi$.
This shows the universal property of exterior powers.

Also, we can consider the ${\bigwedge}^k$ as a functor applied to the map $f : V \to W$.
Then we have ${\bigwedge}^k f : {\bigwedge}^k V \to {\bigwedge}^k W$.
Then the following diagram commutes:
\begin{center}
	\begin{tikzcd}
		\overbrace{V \times V \times \cdots \times V}^{k \text{ times}} \arrow[r, "f \times f \times \cdots \times f"] \arrow[d, hook] & \overbrace{W \times W \times \cdots \times W}^{k \text{ times}} \arrow[d, hook] \\
		{\bigwedge}^k V \arrow[r, "{\bigwedge}^k f", dashed] & {\bigwedge}^k W \\[-3.6em]
		{\scriptscriptstyle \vec{v}_1 \wedge \vec{v}_2 \wedge \cdots \wedge \vec{v}_k} \arrow[r, mapsto] & {\scriptscriptstyle f(\vec{v}_1) \wedge f(\vec{v}_2) \wedge \cdots \wedge f(\vec{v}_k)}
	\end{tikzcd}
\end{center}

Note that the permutation group $S_n$ acts on $\overbrace{V \times V \times \cdots \times V}^{n \text{ times}}$ by:
\[
	\sigma_i : (v_1, v_2, \cdots, v_n) \mapsto (v_1, v_2, \cdots, v_{i - 1}, v_{i + 1}, v_i, v_{i + 2}, \cdots, v_n)
\]
By the universal property of exterior powers, we have:
\begin{center}
	\begin{tikzcd}
		\overbrace{V \times V \times \cdots \times V}^{n \text{ times}} \arrow[r, "\sigma_i"] \arrow[d] & \overbrace{V \times V \times \cdots \times V}^{n \text{ times}} \arrow[d] \\
		{\bigwedge}^n V \arrow[r, "(\sigma_i)_*", dashed] & {\bigwedge}^n V
	\end{tikzcd}
\end{center}

Consider that $a \wedge b = - b \wedge a$.
Then in general, we have:
\[
	P \wedge Q = (-1)^{pq} Q \wedge P
\]
where $P \in {\bigwedge}^p V$ and $Q \in {\bigwedge}^q V$.
This is called the \emph{graded commutativity} of exterior algebras.

\newpage

\section{Determinants and Duals}

Let $V$ be an $n$-dimensional vector spaces over $\F$ and $\B_V = \{ v_1, v_2, \cdots, v_n \}$ be a basis of $V$.

As $\det V$ is a 1-dimensional vector space, so there is a basis.
So the basis of $\det V$ is actually equivalent to $\det V \setminus \{ 0 \}$ Then we have a map from $\B_V$ to $\B_{\det V}$ defined by:
\[
	\vec{v} = (v_1, v_2, \cdots, v_n) \mapsto v_1 \wedge v_2 \wedge \cdots \wedge v_n = \det \vec{v} \in \det V
\]

Then we have the following commutative diagram:
\begin{center}
	\begin{tikzcd}
		\B_{V^*} \arrow[r, "\equiv" description, phantom] \arrow[d] & \B_V \arrow[d] \\
		\B_{\det V^*} \arrow[r, "\equiv" description, phantom] & \B_{\det V}
	\end{tikzcd}
\end{center}

Note that $(\det v)^* \equiv \det v$ where $v \in \B_V$.
So we have the following equivalence:
\[
	\det v^* \equiv \det v \equiv (\det v)^*
\]
The first equivalence is because of the commutative diagram above, and the second equivalence is because of the definition of dual basis.

Consider $L$ be a line over $\F$ and $L^n$ defined as $\overbrace{L \otimes L \otimes \cdots \otimes L}^{n \text{ times}}$.
Also, $L^0$ is defined as $\F$.
Normally, we have $L^* \otimes L \to \F$.
However, as $L$ is 1-dimensional, we have the following isomorphism:
\[
	L^* \otimes L \equiv \F
\]
Then $L^*$ is regarded as $L^{-1}$, and they from a group under the tensor product operation, $(\{ L^k \}, \otimes)$ where $k \in \Z$.

Consider $V_1$ and $V_2$ are two $n$-dimensional vector spaces over $\F$.
Then we have the following diagram:
\begin{center}
	\begin{tikzcd}[column sep=normal]
		V_1 \arrow[rr, "f"] & \arrow[d, Rightarrow, shorten >= 1ex, "\det"'] & V_2 \arrow[r, Rightarrow, "(-)^*"] & V_1^* & \arrow[d, Rightarrow, shorten >= 1ex, "\det"] & V_2^* \arrow[ll, "f^*"'] \\
		\det V_1 \arrow[rr, "\det f"] & \phantom{*} & \det V_2 \arrow[r, dashed, Rightarrow] & \det V_1^* & \phantom{*} & \det V_2^* \arrow[ll, "\det f^*"']
	\end{tikzcd}
\end{center}

Then we consider the left part, we have:
\[
	\det f \in \Hom(\det V_1, \det V_2) \equiv (\det V_1)^* \otimes \det V_2
\]
Similarly, for the right part, we have:
\[
	\det f^* \in (\det V_2^*)^* \otimes \det V_1^* \equiv (\det V_2)^** \otimes \det V_1^* \equiv \det V_2 \otimes (\det V_1)^*
\]
Note that the first equivalence is due to $\det V^* \equiv (\det V)^*$.
As the tensor product is commutative, we have:
\[
	\det f^* \equiv \det f
\]

\newpage

\section{Determinant Formula}

Consider the following diagram:
\begin{center}
	\begin{tikzcd}[column sep=normal]
		\F^n \arrow[rr, "A"] & \arrow[d, Rightarrow, shorten >= 1ex, "\det"'] & \F^n \\
		\det \F^n \arrow[rr, "\det A"] & \phantom{*} & \det \F^n
	\end{tikzcd}
\end{center}
Given the standard basis $\B = \{ \vec{e}_1, \vec{e}_2, \cdots, \vec{e}_n \}$ of $\F^n$, we have:
\[
	\det \B = \vec{e}_1 \wedge \vec{e}_2 \wedge \cdots \wedge \vec{e}_n
\]
Note that
\[
	A = \begin{bmatrix}
		| & | & & | \\
		\vec{a}_1 & \vec{a}_2 & \cdots & \vec{a}_n \\
		| & | & & |
	\end{bmatrix}
\]
Consider the map $\det A : \det \B \mapsto \det A \cdot \det \B$ where $\det A \in \F$ is a scalar, we have
\[
	\det A \cdot \det \B = A\vec{e}_1 \wedge A\vec{e}_2 \wedge \cdots \wedge A\vec{e}_n = \vec{a}_1 \wedge \vec{a}_2 \wedge \cdots \wedge \vec{a}_n
\]
So, we know that $\det A$ is multilinear and alternating in the columns of $A$.
Also, $\det I = 1$.

Consider the elements of $A$ as $\vec{a}_j = \sum_{i_j = 1}^n a_j^{i_j} \vec{e}_{i_j}$ for all $1 \leq j \leq n$.
Then we have:
\[
	\vec{a}_1 \wedge \cdots \wedge \vec{a}_n = \sum_{i_1 = 1}^n a_1^{i_1} \vec{e}_{i_1} \wedge \cdots \wedge \sum_{i_n = 1}^n a_n^{i_n} \vec{e}_{i_n} = \sum_{i_1, \cdots, i_n = 1}^n a_1^{i_1} \cdots a_n^{i_n} \ (\vec{e}_{i_1} \wedge \cdots \wedge \vec{e}_{i_n})
\]
We assume that $\vec{e}_{i_k}$ are mutually distinct for all $1 \leq k \leq n$.
Otherwise, the term is $0$ because of the alternating property of exterior products.
So there exists a unique permutation $\sigma \in S_n$ such that $i_k = \sigma(k)$ for all $1 \leq k \leq n$.
Then we have:
\[
	\vec{a}_1 \wedge \cdots \wedge \vec{a}_n = \sum_{\sigma \in S_n} a_1^{\sigma(1)} \cdots a_n^{\sigma(n)} \ (\vec{e}_{\sigma(1)} \wedge \cdots \wedge \vec{e}_{\sigma(n)}) = \sum_{\sigma \in S_n} a_1^{\sigma(1)} \cdots a_n^{\sigma(n)} \ \sgn(\sigma) \ (\vec{e}_1 \wedge \cdots \wedge \vec{e}_n)
\]

Hence, we have the formula of determinants:
\[
	\det A = \sum_{\sigma \in S_n} \sgn(\sigma) \ a_1^{\sigma(1)} a_2^{\sigma(2)} \cdots a_n^{\sigma(n)}
\]
\begin{remark}
	For the magnitude part in the formula, $a_1^{\sigma(1)} a_2^{\sigma(2)} \cdots a_n^{\sigma(n)}$, they are in distinct rows and in distinct columns.
	They are in distinct columns because of the subscript of $a_j^{\sigma(j)}$ is $j$ for all $1 \leq j \leq n$.
	They are in distinct rows due to the $\sigma$, otherwise it will be zero because of the alternating property of exterior products.
\end{remark}

\newpage

\section{Properties of Determinants}

The $\det A$ has the following properties:
\begin{itemize}
	\item Linear in each column: for all $1 \leq j \leq n$;
	\item Alternating (skew-symmetric): $\cdots \vec{a}_i \cdots \vec{a}_j \cdots = - \cdots \vec{a}_j \cdots \vec{a}_i \cdots$ for all $i < j$;
	\item $\det I = 1$;
\end{itemize}
For the alternating property, we have the following evaluation from the original definition of wedge products (we assumed that $\chart(\F) \neq 2$):
\begin{align*}
	\cdots \vec{a}_i \overbrace{\color{red} \cdots}^{k \text{ times}} \vec{a}_j \cdots & = (-1)^k \cdots {\color{red} \cdots} \vec{a}_i \vec{a}_j \cdots \\
	& = (-1)^{k + 1} \cdots {\color{red} \cdots} \vec{a}_j \vec{a}_i \cdots \\
	& = - \cdots \vec{a}_j {\color{red} \cdots} \vec{a}_i \cdots
\end{align*}
Moreover, the three properties above uniquely determine the determinant function.
\begin{remark}
	The first two properties can be defined on the rows of $A$ as well and they still hold.
	This is because the determinant of a matrix is equal to the determinant of its transpose, which is the matrix part of $\det f^* \equiv \det f$ shown in the previous section.
\end{remark}

If we drop the last property, then the function is called the \emph{alternating multilinear form}.
Suppose that $\phi : \M{n \times n}{\F} \to \F$ is an alternating multilinear form, then we have:
\[
	\phi(A) = \det A \phi(I_n)
\]

\begin{proposition}
	The following equality holds:
	\[
		\det \begin{bmatrix}
			A_1 & * \\
			0 & A_2
		\end{bmatrix} = \det A_1 \cdot \det A_2
	\]
\end{proposition}
\begin{proof}
	Consider the part on the left-hand side, we know that it is multilinear in the columns and alternating.
	Then we have the following evaluation:
	\begin{align*}
		\det \begin{bmatrix}
			A_1 & * \\
			0 & A_2
		\end{bmatrix} & = \det A_1 \cdot \det \begin{bmatrix}
			I_{n_1} & * \\
			0 & A_2
		\end{bmatrix} \\
		& = \det A_1 \cdot \det A_2 \cdot \det \begin{bmatrix}
			I_{n_1} & * \\
			0 & I_{n_2}
		\end{bmatrix} \\
		& = \det A_1 \cdot \det A_2 \cdot \det \begin{bmatrix}
			I_{n_1} & 0 \\
			0 & I_{n_2}
		\end{bmatrix} \\
		& = \det A_1 \cdot \det A_2 \cdot \det I_{n_1 + n_2} = \det A_1 \cdot \det A_2
	\end{align*}
	For the last equality, as we know the following property:
	\[
		\cdots \vec{a}_i \cdots (k\vec{a}_i + \vec{a}_j) \cdots = k \cdots \vec{a}_i \cdots \vec{a}_j \cdots + \cdots \vec{a}_i \cdots \vec{a}_j \cdots = \cdots \vec{a}_i \cdots \vec{a}_j \cdots
	\]
	Note that $k$ can be 0 as well.
	Therefore, we can eliminate all the $*$ in the matrix by using the above property without changing the determinant value.
\end{proof}

Instead of writing $\det$, we can use two pipes to denote the determinant.
Concretely, we have the following determinants:
\[
	\left|
		\begin{array}{cc|ccc}
			1 & & * & * & * \\
			& 1 & * & * & * \\
			\hline
			& & 1 & & \\
			& & & 1 & \\
			& & & & 1
		\end{array}
	\right| = \left|
		\begin{array}{cc|ccc}
			1 & & 0 & 0 & 0 \\
			& 1 & * & * & * \\
			\hline
			& & 1 & & \\
			& & & 1 & \\
			& & & & 1
		\end{array}
	\right| = \left|
		\begin{array}{cc|ccc}
			1 & & 0 & 0 & 0 \\
			& 1 & 0 & 0 & 0 \\
			\hline
			& & 1 & & \\
			& & & 1 & \\
			& & & & 1
		\end{array}
	\right| = |I_5| = 1
\]
For the first equality, we eliminated the first row's $*$ by using the first row.
For the second equality, we eliminated the second row's $*$ by using the second row.

So, for block upper-triangular matrices, its determinant is equal to the product of the determinants of the diagonal blocks.
Same for the block lower-triangular matrices.

In particular, we have the following equation:
\[
	\begin{vmatrix}
		a_{11} & \cdots & * \\
		\vdots & \ddots & \vdots \\
		0 & \cdots & a_{nn}
	\end{vmatrix} = a_{11} \cdots a_{nn}
\]
Also, $\det [a] = a \det[1] = a$.
\begin{remark}
	In determinant, we prefer to use $a_{ij}$ to denote the element in the $i$-th row and $j$-th column instead of using superscript and subscript like $a_j^{i}$.
	This is because in determinants, we usually consider the rows and columns instead of vectors.
\end{remark}

Consider the following determinant:
\vspace{6ex}
\begin{align*}
	\begin{vmatrix}
		& & & \mypoint{jcol}{\phantom{*}} \\
		& * & & 0 & & * & \\
		\\
		\mypoint{irow}{a_{i,1}} & \cdots & a_{i,j-1} & 1 & a_{i,j+1} & \cdots & a_{i,n} \\
		\\
		& * & & 0 & & * & \\
		\\
	\end{vmatrix} & = (-1)^{i - 1} \begin{vmatrix}
		a_{i,1} & \cdots & a_{i,j-1} & 1 & a_{i,j+1} & \cdots & a_{i,n} \\
		\\ \\
		& * & & 0 & & * & \\
		\\ \\
	\end{vmatrix} \\
	& = (-1)^{i - 1 + j - 1} \begin{vmatrix}
		1 & a_{i,1} & \cdots & \widehat{a_{i,j}} & \cdots & a_{i,n} \\
		\\ \\
		0 & & & A^i_j \\
		\\ \\
	\end{vmatrix} \\
	& = (-1)^{i + j} \det A^i_j
\end{align*}

\begin{tikzpicture}[remember picture, overlay]
	\node[above=20pt of jcol](textofhere1){the $j$-th column};
	\draw[myarrow] (textofhere1) -- (jcol);
	\node[left=20pt of irow](textofhere2){the $i$-th row};
	\draw[myarrow] (textofhere2) -- (irow);
\end{tikzpicture}
Here $\widehat{a_{i,j}}$ means that the element $a_{i,j}$ is omitted, and $A^i_j$ is the submatrix obtained by deleting the $i$-th row and $j$-th column of $A$.

Then we can consider general matrix $A$, for any $j$, we have:
\begin{align*}
	\det A & = \det [\cdots \quad \vec{a}_j \quad \cdots] \\
	& = \sum_{i = 1}^n a_j^i \det [\cdots \quad \vec{e}_i \quad \cdots] \\
	& = \sum_{i = 1}^n a_j^i (-1)^{i + j} \det A^i_j
\end{align*}
This is called the \emph{cofactor expansion} or \emph{Laplace expansion} along the $j$-th column.
Similarly, we can have the cofactor expansion along the $i$-th row.

Then we have the definition of \emph{adjoint}\index{adjoint} of a matrix.
\begin{definition}[Adjoint Matrices]
	An \emph{adjoint matrix} of $A$, denoted by $\adj A$, is defined as the matrix whose $(i,j)$-th entry is $(-1)^{i + j} \det A_i^j$.
\end{definition}
\begin{remark}
	Beaware of the notation difference between $A_i^j$ and $A^i_j$.
	The former means deleting the $j$-th row and $i$-th column, while the latter means deleting the $i$-th row and $j$-th column.
	Also note that the notation of $\vec{e}_i$ means that the $i$-th row is 1 and other rows are 0 (standard basis vector), which is different from the notation in $A_i^j$ and $A^i_j$.
	To conclude, the subscript is for columns and the superscript is for rows, except they are in the notation of standard basis vectors.
\end{remark}

\begin{proposition}
	The following equality holds:
	\[
		A \cdot \adj A = \adj A \cdot A = \det A I_n
	\]
	In particular, if $\det A \neq 0$, then $A^{-1} = \frac{1}{\det A} \adj A$.
\end{proposition}
\begin{proof}
	In particular, we just have to show
	\[
		\sum_{k = 1}^n a^k_j (\adj A)^i_k = \det A \delta^i_j
	\]

	From the previous Laplace expansion, we know:
	\[
		\det A = \sum_{i = 1}^n a_j^i (-1)^{i + j} \det A^i_j = \sum_{i = 1}^n a_j^i (\adj A)_i^j = (A \cdot \adj A)_j^j
	\]
	Then we know that for $i = j$, the equality holds.
	If $i \neq j$, then we can consider the following determinant:
	\vspace{4ex}
	\[
		\det \begin{vmatrix}
			\\
			\cdots & \mypoint{icol}{\vec{a}_j} & \cdots & \mypoint{jcol}{\vec{a}_j} & \cdots \\
			\\
		\end{vmatrix} = 0
	\]
	\vspace{2ex}

	This means that originally, there are two same columns in the determinant, so its value is zero.
	Then by the Laplace expansion along the $j$-th column, we have:
	\[
		0 = \sum_{k = 1}^n a_j^k (-1)^{k + j} \det A^k_i = \sum_{k = 1}^n a_j^k (\adj A)_k^i = (A \cdot \adj A)_j^i
	\]
\end{proof}
\begin{tikzpicture}[remember picture, overlay]
	\node[below=20pt of icol](textofhere1){the $i$-th column};
	\draw[myarrow] (textofhere1) -- (icol);
	\node[above=20pt of jcol](textofhere2){the $j$-th column};
	\draw[myarrow] (textofhere2) -- (jcol);
\end{tikzpicture}

To better understand the reason why the equality holds when $i \neq j$, we can consider the following explanation~\cite{1404250}.
Consider the $3 \times 3$ case:
\[
	\underbrace{\begin{bmatrix}
		A_1^1 & -A_1^2 & A_1^3 \\
		-A_2^1 & A_2^2 & -A_2^3 \\
		A_3^1 & -A_3^2 & A_3^3
	\end{bmatrix}}_{\adj A} \cdot \underbrace{\begin{bmatrix}
		{\color{red} a_1^1} & {\color{ocre} a_2^1} & a_3^1 \\
		{\color{red} a_1^2} & {\color{ocre} a_2^2} & a_3^2 \\
		{\color{red} a_1^3} & {\color{ocre} a_2^3} & a_3^3
	\end{bmatrix}}_{A}
\]
If we multiply the first row of $\adj A$ with the first column of $A$, we have the same result as the Laplace expansion along the first column:
\[
	{\color{red} a_1^1} A_1^1 - {\color{red} a_1^2} A_1^2 + {\color{red} a_1^3} A_1^3 = \begin{vmatrix}
		{\color{red} a_1^1} & a_2^1 & a_3^1 \\
		{\color{red} a_1^2} & a_2^2 & a_3^2 \\
		{\color{red} a_1^3} & a_2^3 & a_3^3
	\end{vmatrix} = \det A = \sum_{k = 1}^3 {\color{red} a_1^k} A_1^k = \sum_{k = 1}^3 {\color{red} a_1^k} (\adj A)_k^1
\]
If we multiply the first row of $\adj A$ with the second column of $A$, we have:
\[
	{\color{ocre} a_2^1} A_1^1 - {\color{ocre} a_2^2} A_1^2 + {\color{ocre} a_2^3} A_1^3 = \begin{vmatrix}
		{\color{ocre} a_2^1} & a_2^1 & a_3^1 \\
		{\color{ocre} a_2^2} & a_2^2 & a_3^2 \\
		{\color{ocre} a_2^3} & a_2^3 & a_3^3
	\end{vmatrix} = 0 = \sum_{k = 1}^3 {\color{ocre} a_2^k} A_1^k = \sum_{k = 1}^3 {\color{ocre} a_2^k} (\adj A)_k^1
\]

\newpage

\section{Vandermonde Determinant}

Consider the following determinant, here the superscript means the power:
\[
	\det V_n = \begin{vmatrix}
		1 & 1 & \cdots & 1 \\
		x_1 & x_2 & \cdots & x_n \\
		\vdots & \vdots & \ddots & \vdots \\
		x_1^{n - 1} & x_2^{n - 1} & \cdots & x_n^{n - 1}
	\end{vmatrix}
\]
Then we consider $x_1, x_2, \cdots, x_{n - 1}$ are fixed and we consider the determinant as a polynomial of $x_n$.
Note that the degree of $x_n$ is $n - 1$, and the polynomial is:
\[
	\det V_n = (-1)^{n + 1} | \cdots | + (-1)^{n + 2} x_n | \cdots | + \cdots + (-1)^{n + n} x_n^{n - 1} \begin{vmatrix}
		1 & 1 & \cdots & 1 \\
		x_1 & x_2 & \cdots & x_{n - 1} \\
		\vdots & \vdots & \ddots & \vdots \\
		x_1^{n - 1} & x_2^{n - 1} & \cdots & x_{n - 1}^{n - 1}
	\end{vmatrix}
\]
Also note that if $x_n = x_i$ for some $1 \leq i \leq n - 1$, let say $i = n - 1$, then the determinant becomes:
\[
	\begin{vmatrix}
		1 & 1 & \cdots & 1 & 1 \\
		x_1 & x_2 & \cdots & x_{n - 1} & x_{n - 1} \\
		\vdots & \vdots & \ddots & \vdots & \vdots \\
		x_1^{n - 1} & x_2^{n - 1} & \cdots & x_{n - 1}^{n - 1} & x_{n - 1}^{n - 1}
	\end{vmatrix} = 0
\]
This means that $x_n - x_i$ is a factor of the polynomial.
Therefore, by the fundamental theorem of algebra, we have:
\[
	\det V_n = C \overbrace{(x_n - x_1)(x_n - x_2) \cdots (x_n - x_{n - 1})}^{n - 1 \text{ factors}}
\]
Here $C$ is a constant that does not depend on $x_n$.
To find $C$, we can consider the coefficient of $x_n^{n - 1}$.
Note that the coefficient of $x_n^{n - 1}$ in the above polynomial expansion is $\det V_{n - 1}$.
So $C = \det V_{n - 1}$.
Then by induction, we have:
\[
	\det V_n = \prod_{1 \leq i < j \leq n} (x_j - x_i)
\]

\newpage

\section{Feynman Diagram Formula}

Consider the $\chart(\F) = 0$.
Let $A$ be an $n \times n$ matrix and $I$ be the identity matrix of order $n$.
Then we have the following formula:
\[
	\det (I + tA) = 1 - \tr A \ t + \left(\frac{(\tr A)^2}{2!} - \frac{\tr A^2}{2}\right) t^2 - \cdots + (-1)^n \det A \ t^n
\]
This is called the \emph{Feynman diagram formula}, as it is inspired by Feynman diagrams in quantum field theory.
From this formula, the determinant can be expressed by traces.

It is hard to remember the coefficients in the formula.
However, we can use the following method to derive them.
Consider the following diagram for $t^1$ term:
\begin{center}
	\begin{tikzpicture}
		\draw (0, 0) circle (0.5cm) node {$-1$}
			[arrow inside={end=stealth,opt={ocre,scale=1}}{0}];
		\path (0.5, 0) node[right, ocre] {$A$};
	\end{tikzpicture}
\end{center}
Here the circle means a trace operation, and the arrow means $A$.
So the coefficient is $- \tr A$.

For $t^2$ term, we have diagram:
\begin{center}
	\begin{tikzpicture}
		\draw (0, 0) circle (0.5cm) node {$-1$}
			[arrow inside={end=stealth,opt={ocre,scale=1}}{0}];
		\path (0.5, 0) node[right, ocre] {$A$};
		\draw (2, 0) circle (0.5cm) node {$-1$}
			[arrow inside={end=stealth,opt={ocre,scale=1}}{0}];
		\path (2.5, 0) node[right, ocre] {$A$};

		\draw (5, 0) circle (1cm) node {$-1$}
			[arrow inside={end=stealth,opt={ocre,scale=1}}{0,0.5}];
		\path (6, 0) node[right, ocre] {$A$}
			(4, 0) node[left, ocre] {$A$}
			(5, 1) node {$|$}
			(5, -1) node {$|$};
	\end{tikzpicture}
\end{center}
The left two circles mean $(- \tr A)^2$, and we have to divide by $2!$ because of the symmetry of the two identical circles.
The right circle means $- \tr A^2$, but this is a cyclic group of order 2, so we have to divide by $2$.
Therefore, the total term for $t^2$ is:
\[
	\frac{(- \tr A)^2}{2!} - \frac{\tr A^2}{2} = \frac{(\tr A)^2}{2!} - \frac{\tr A^2}{2}
\]

For $t^3$ term, we have diagram:
\begin{center}
	\begin{tikzpicture}
		\draw (-2, 0) circle (0.5cm) node {$-1$}
			[arrow inside={end=stealth,opt={ocre,scale=1}}{0}];
		\path (-1.5, 0) node[right, ocre] {$A$};
		\draw (0, 0) circle (0.5cm) node {$-1$}
			[arrow inside={end=stealth,opt={ocre,scale=1}}{0}];
		\path (0.5, 0) node[right, ocre] {$A$};
		\draw (2, 0) circle (0.5cm) node {$-1$}
			[arrow inside={end=stealth,opt={ocre,scale=1}}{0}];
		\path (2.5, 0) node[right, ocre] {$A$};

		\draw (5, 0) circle (1cm) node {$-1$}
			[arrow inside={end=stealth,opt={ocre,scale=1}}{0,0.5}];
		\path (6, 0) node[right, ocre] {$A$}
			(4, 0) node[left, ocre] {$A$}
			(5, 1) node {$|$}
			(5, -1) node {$|$};
		\draw (7.5, 0) circle (0.5cm) node {$-1$}
			[arrow inside={end=stealth,opt={ocre,scale=1}}{0}];
		\path (8, 0) node[right, ocre] {$A$};

		\draw (10.5, 0) circle (1.5cm) node {$-1$}
			[arrow inside={end=stealth,opt={ocre,scale=1}}{0.0833, 0.4167, 0.755}];
		\path (11.8, 0.75) node[right, ocre] {$A$}
			(9.2, 0.75) node[left, ocre] {$A$}
			(10.5, -1.5) node[below, ocre] {$A$}
			(10.5, 1.5) node {$|$}
			(11.8, -0.75) node[rotate=240] {$|$}
			(9.2, -0.75) node[rotate=120] {$|$};
	\end{tikzpicture}
\end{center}
The left three circles mean $(- \tr A)^3$, and we have to divide by $3!$ because of the symmetry of the three identical circles.
The second diagram means $(- \tr A)(- \tr A^2)$, and we have to divide by $2$ because of the cyclic group of order $2$ on the bigger circle.
The last diagram means $- \tr A^3$, and this is a cyclic group of order 3, so we have to divide by $3$.
Therefore, the total term for $t^3$ is:
\[
	\frac{(- \tr A)^3}{3!} + \frac{(- \tr A)(- \tr A^2)}{2} - \frac{\tr A^3}{3} = - \frac{(\tr A)^3}{3!} + \frac{(\tr A)(\tr A^2)}{2} - \frac{\tr A^3}{3}
\]
