%----------------------------------------------------------------------------------------
%	CHAPTER 1
%----------------------------------------------------------------------------------------

\chapter{Abstract Linear Spaces}

\epigraph{``I assume you have learnt linear algebra.''}{Guowu Meng} % sybau

\section{Binary Operation}

We start with the definition of a binary operation.

\begin{definition}[Binary Operation]
	A \emph{binary operation}\index{binary operation} on a set $S$ is a mapping of the elements of the Cartesian product $S \times S$ to $S$.
	\begin{align*}
		\cdot : S \times S & \to S \\ (x,y) &\mapsto x \cdot y
	\end{align*}
\end{definition}

For easier understanding, binary operation is combining two objects into one.
Hence, there is something called unary and ternary operations, corresponding to the action of combining one and three objects into one respectively.

\begin{example}
	A common example of a binary operation is addition on the set of natural numbers $\mathbb{N}$.
	\begin{equation}
		\begin{split}
			+ : \mathbb{N} \times \mathbb{N} & \to \mathbb{N} \\ (x,y) &\mapsto x+y
		\end{split}
	\end{equation}
\end{example}

\begin{definition}[Associative]
	A binary operation $\cdot: S \times S \to S$ is \emph{associative}\index{associative} if, for all $x,y,z \in S$,
	\[ x \cdot (y \cdot z) = (x \cdot y) \cdot z \]
\end{definition}

\begin{example}
	A common example of an associative (binary) operation is addition on the set of natural numbers $\mathbb{N}$.
	For all $x,y,z \in \mathbb{N}$, we have $x + (y + z) = (x + y) + z$.
\end{example}

\begin{definition}[Unital]
	A binary operation $\cdot: S \times S \to S$ is \emph{unital}\index{unital} if there exists an element $e \in S$, the \emph{identity}\index{identity} or \emph{unit element}\index{unit element}, such that, for all $x \in S$
	\[ e \cdot x = x = x \cdot e \]
\end{definition}

\begin{example}
	A common example of an unitary binary operation is multiplication on the set of natural numbers $\mathbb{N}$.
	The identity element is $1$, and for all $x \in \mathbb{N}$, we have $x \cdot 1 = x = 1 \cdot x$.
\end{example}

\begin{proposition}
	The identity element of a unital binary operation is unique.
\end{proposition}

\begin{proof}
	If $e_1$ and $e_2$ are two identity elements for the operation $\cdot$, then by definition, we have
	\[ e_1 = e_1 \cdot e_2 = e_2. \]
\end{proof}

Note that the two-sided identity must be unique, but one-sided identities need not be.
The following is an example of it.

\begin{example}
	Consider a set $X = \left\{ \begin{bmatrix}
		1 & a \\
		0 & 0
	\end{bmatrix} \; \middle| \; a \in \R \right\}$ with the binary operation defined as matrix multiplication.
	This set has many left identity elements, but no two-sided identity element.
\end{example}

\begin{definition}[Invertible]
	A unital binary operation $\cdot: S \times S \to S$ is \emph{invertible}\index{invertible} if, for every element $x \in S$, there exists an element $y \in S$, called the two-sided \emph{inverse}\index{inverse} of $x$, denoted as $x^{-1}$, such that
	\[ x \cdot y = e = y \cdot x \]
	where $e$ is the identity element of the operation.
\end{definition}
\begin{remark}
	An invertible operation must be unital, since the identity element is required in the definition of invertibility.
\end{remark}

\begin{example}
	A common example of an invertible (binary) operation is addition on the set of integers $\Z$.
	For every integer $x \in \Z$, there exists an integer $y = -x$ such that:
	\begin{equation}
		x + (-x) = 0 = (-x) + x
	\end{equation}
	where $0$ is the identity element for addition.
\end{example}

\begin{proposition}
	The inverse element of an invertible operation is unique.
\end{proposition}

\begin{proof}
	Let $y_1$ and $y_2$ be two inverses of an element $x \in S$.
	Then, by definition of inverse, we have:
	\begin{gather*}
		x \cdot y_1 = e = y_1 \cdot x\\
		x \cdot y_2 = e = y_2 \cdot x
	\end{gather*}
	Now, consider the element $y_1$: $y_1 \cdot x = e$.
	But since $y_2$ is also an inverse of $x$, we can substitute $e$ with $x \cdot y_2$: $y_1 \cdot x = x \cdot y_2 = e$.
	By the associativity of the operation, we can rearrange this to:
	\[ y_1 = y_1 \cdot e = y_1 \cdot (x \cdot y_2) = (y_1 \cdot x) \cdot y_2 = e \cdot y_2 = y_2 \]
	Thus, the inverse element is unique.
\end{proof}

Same for the identity, a one-sided inverse need not be unique.
The example is left as an exercise.

\begin{definition}[Commutative]
	A binary operation $\cdot: S \times S \to S$ is \emph{commutative}\index{commutative} if, for all $x,y \in S$, the following holds:
	\[ x \cdot y = y \cdot x \]
\end{definition}

\begin{example}
	A common example of a commutative operation is addition on the set of integers $\Z$.
	For all $x,y \in \Z$, we have: $x + y = y + x$.
\end{example}

\begin{definition}[Distributive (Harmonic)]
	A binary operation $\cdot: S \times S \to S$ is \emph{distributive}\index{distributive} with respect to another binary operation $+: S \times S \to S$ if, for all $x,y,z \in S$, the following holds:
	 \begin{align*}
		x \cdot (y + z) &= x \cdot y + x \cdot z, \\
		(y + z) \cdot x &= y \cdot x + z \cdot x.
	\end{align*}
\end{definition}

The professor preferred to use the word ``harmonic'' instead of ``distributive''.
Note that it is important to show that ``\emph{which binary operation} is distributive to \emph{which binary operation}''.
(The two binary operation in this sentence is not commutative.)

\begin{example}
	A common example of a distributive operation is multiplication over addition on the set of integers $\Z$.
	For all $x,y,z \in \Z$, we have:
	 \begin{align*}
		x \cdot (y + z) &= x \cdot y + x \cdot z, \\
		(y + z) \cdot x &= y \cdot x + z \cdot x.
	\end{align*}
\end{example}

\newpage

\section{Groups, Rings and Fields}

With those five properties, we can construct monoid and groups.

\begin{definition}[Monoid]
	A \emph{monoid}\index{monoid} is a set $M$ equipped with an associative unital binary operation $f: M \times M \to M$.
	We say $(M, f)$ is a monoid, and $f$ is the \emph{monoid operation}\index{monoid operation} on the set $M$.
	A set $M$ with a monoid operation $f$ is the \emph{monoid structure}\index{monoid structure}.
\end{definition}

\begin{definition}[Group]
	A \emph{group}\index{group} is a set $G$ equipped with a monoid operation $f: G \times G \to G$ with the additional property that every element has an inverse.
\end{definition}

\begin{example}
	$(\R\setminus\{0\}, \times)$ is a group, but $(\R, \times)$ is not a group since $0$ does not have a multiplicative inverse.
\end{example}

\begin{definition}[Abelian Monoid / Group]
	A monoid / group $(S, f)$ is an \emph{abelian}\index{abelian} if the operation $f$ is commutative.
\end{definition}

\begin{definition}[Unital Ring]
	A \emph{unital ring}\index{unital ring} is a set $R$ equipped with two binary operations $f: R \times R \to R$ (addition) and $g: R \times R \to R$ (multiplication) such that the following properties hold:
	\begin{enumerate}
		\item \emph{Additive Group:} $(R, f)$ is an abelian group.
		\item \emph{Multiplicative Monoid:} $(R, g)$ is a monoid.
		\item \emph{Distributive Property:} $g$ with respect to $f$.
	\end{enumerate}
\end{definition}

\begin{definition}[Commutative Ring]
	A \emph{commutative ring}\index{commutative ring} is a unital ring $R$ such that the multiplication operation $g: R \times R \to R$ is commutative.
\end{definition}

\begin{example}
	$(\Z, +, \times)$ is a commutative ring.
\end{example}

\begin{definition}[Field]
	A \emph{field}\index{field} is a commutative ring $\mathbb{k}$ such that every non-zero element has a multiplicative inverse.
\end{definition}

\begin{example}
	$(\mathbb{Q}, +, \times)$, $(\R, +, \times)$ and $(\mathbb{C}, +, \times)$ are fields.
\end{example}

\begin{example}[Finite Field]
	$(\Z/2\Z, +, \times)$ is a field, where $\Z/2\Z = \{[0], [1]\}$, $[0]$ is the set of even integers and $[1]$ is the set of odd integers.
	Note that any $\Z/p\Z$ is a finite field, where $p$ is a prime number.
\end{example}

We may draw a diagram for the relationship between the algebraic structures.

\begin{center}
	\begin{tikzcd}[column sep=3.5em, row sep=huge, math mode=false]
		Set \arrow[r, "\scriptsize Closed Operation"] &
		Magma \arrow[r, "\scriptsize Associativity"] &
		Semigroup \arrow[d, "\scriptsize Identity"] \\

		Abelian Group &
		Group \arrow[swap, l, "\scriptsize Commutative"] &
		Monoid \arrow[swap, l, "\scriptsize Inverse"] \arrow[r, "\scriptsize Commutative"] \arrow[dl, "$\times$", ocre] &
		Abelian Monoid \arrow[dl, red, "$\times$", swap] \\

		Rng \arrow[from=ur, "$+$" near end, swap] \arrow[from=urr]
		& {\color{ocre} Unital Ring} \arrow[ocre, from=ul, crossing over, "$+$" near start, swap] \arrow[r]
		& {\color{red} Commutative Ring} \arrow[from=ull, crossing over, red, "$+$", swap] \arrow[r, "\scriptsize Multiplicative", "\scriptsize Inverse" swap]
		& Field
	\end{tikzcd}
\end{center}

\newpage

\section{Morphisms}

Normally, when we have two sets we can have a set map.
Then if the two are in the same algebraic structures?
They are called the homomorphisms.

\begin{definition}[Monoid Homomorphism]
	A \emph{monoid homomorphism}\index{monoid homomorphism} is a morphism between two monoids that preserves the monoid structure.
	Formally, let $(M_1, \cdot_1)$ and $(M_2, \cdot_2)$ be two monoids with identity elements $e_1$ and $e_2$, respectively.
	A function $f: M_1 \to M_2$ is a monoid homomorphism if:
	\begin{enumerate}
		\item $f(x \cdot_1 y) = f(x) \cdot_2 f(y) \quad \forall x, y \in M_1$
		\item $f(e_1) = e_2$
	\end{enumerate}
\end{definition}

\begin{definition}[Group Homomorphism]
	Let $(G_1, \cdot_1)$ and $(G_2, \cdot_2)$ be two groups with identity elements $e_1$ and $e_2$, respectively.
	A function $f: G_1 \to G_2$ is a \emph{group homomorphism}\index{group homomorphism} if:
	\begin{enumerate}
		\item $f(x \cdot_1 y) = f(x) \cdot_2 f(y) \quad \forall x, y \in G_1$
		\item $f(e_1) = e_2$
		\item $f(x^{-1}) = (f(x))^{-1} \quad \forall x \in G_1$
	\end{enumerate}
\end{definition}

\begin{proposition}
	The second and third are consequences of the first.
\end{proposition}

\begin{proof}
	Let $f: G_1 \to G_2$ be a map satisfying the first property.

	\textbf{Second Property:} For any element $x \in G_1$, we have:
	\[
		f(x) = f(x \cdot_1 e_1) = f(x) \cdot_2 f(e_1)
	\]
	So for any $f(x) \in G_2$, this implies that $f(e_1)$ must be the identity element in $G_2$, i.e., $f(e_1) = e_2$.

	\textbf{Third Property:} We have:
	\[
		e_2 = f(e_1) = f(x \cdot_1 x^{-1}) = f(x) \cdot_2 f(x^{-1})
	\]
	This shows that $f(x^{-1})$ is the inverse of $f(x)$ in $G_2$, i.e., $f(x^{-1}) = (f(x))^{-1}$.
\end{proof}

For monoid homomorphisms, the second property cannot be derived from the first property.
Consider the identity element $e_1$ in $M_1$.
If we apply the first property, we get $f(e_1 \cdot_1 e_1) = f(e_1) \cdot_2 f(e_1)$.
This simplifies to $f(e_1) = f(e_1) \cdot_2 f(e_1)$, which does not necessarily imply that $f(e_1)$ is the identity element in $M_2$, i.e., $f(e_1) \neq e_2$, but $f(e_1)$ is the idempotent element in $M_2$.
Therefore, the second property must be explicitly stated for monoid homomorphisms.

However in the case of group homomorphisms, the existence of inverses ensures that there is only one element that can be idempotent under the group operation, which is the identity element.
Thus, for group homomorphisms, the second property can be derived from the first property.

\begin{definition}[Idempotent Elements]
	An element $a$ is \emph{idempotent}\index{idempotent} if $a = a^2$.
\end{definition}

\newpage

To introduce the vector space, the following two morphisms are required.

\begin{definition}[Ring Homomorphism]
	A \emph{ring homomorphism}\index{ring homomorphism} is a morphism between two rings that preserves both the additive and multiplicative structures.
	Formally, let $(R_1, +_1, \cdot_1)$ and $(R_2, +_2, \cdot_2)$ be two rings with identity elements $0_1$, $1_1$ and $0_2$, $1_2$, respectively.
	A function $f: R_1 \to R_2$ is a ring homomorphism if:
	\begin{enumerate}
		\item $f(x +_1 y) = f(x) +_2 f(y) \quad \forall x, y \in R_1$
		\item $f(x \cdot_1 y) = f(x) \cdot_2 f(y) \quad \forall x, y \in R_1$
		\item $f(1_1) = 1_2$
	\end{enumerate}
\end{definition}

\begin{definition}[Endomorphism]
	An \emph{endomorphism}\index{endomorphism} is a morphism from an algebraic structure to itself.
	Formally, let $(A, \cdot)$ be an algebraic structure.
	An endomorphism $f: A \to A$ is a set map such that:
	\[
		f(x \cdot y) = f(x) \cdot f(y) \quad \forall x, y \in A
	\]
\end{definition}

The following two sets are the sets of all structure-preserving maps.

\begin{definition}[Hom-set]
	The set of all morphisms from an algebraic structure $A$ to another algebraic structure $B$ is called the \emph{hom-set}\index{hom-set}, denoted by $\Hom(A, B)$.
\end{definition}

\begin{definition}[Endomorphism Ring]
	The set of all endomorphisms of an abelian group $(A, +)$, denoted by $\End(A)$, forms a (non-commutative) ring under pointwise addition and composition of set maps.
	The addition and multiplication operations are defined as follows:
	\begin{align*}
		+ : \End(A) \times \End(A) &\to \End(A) \\
		(f,g) &\mapsto (f+g: x \mapsto f(x) + g(x)) \qquad &f + g : A \to A \\
		\circ : \End(A) \times \End(A) &\to \End(A) \\
		(f,g) &\mapsto (f \circ g: x \mapsto f(g(x))) \qquad &f \circ g : A \to A
	\end{align*}
	The identity element for addition is the zero endomorphism, which maps every element to the identity element of the group.
	\begin{align*}
		0: A &\to A \\
		x &\mapsto 0
	\end{align*}
	The identity element for multiplication is the identity endomorphism, which maps every element to itself.
	\begin{align*}
		1: A &\to A \\
		x &\mapsto x
	\end{align*}
	Note that all endomorphisms in $\End(A)$ are group homomorphisms and $\End(A) = \Hom(A, A)$.
\end{definition}

\newpage

\section{Linear Spaces}

Then we can define what a linear structure is.

\begin{definition}[Linear Structure]
	A \emph{linear structure}\index{linear structure} over a field $\mathbb{k}$ on a set $V$ is a pair $(+, \cdot)$ where $(V, +)$ is an abelian group with a ring homomorphism $\mathbb{k} \to \End(V)$, where $\End(V)$ is the endomorphism ring of the abelian group $(V, +)$.
	\begin{align*}
		\cdot : \mathbb{k} &\to \End(V) \\
		\alpha &\mapsto (\alpha\cdot : \vec x \mapsto \alpha \vec x) \qquad \alpha \cdot : V \to V
	\end{align*}
	The ring homomorphism is a (ring) action of the field $\mathbb{k}$ on the abelian group $(V, +)$, called \emph{scalar multiplication}\index{scalar multiplication}.
	The ring action can be written as a binary operation:
	\begin{align*}
		\cdot : \mathbb{k} \times V &\to V \\
		(\alpha, \vec x) &\mapsto \alpha \vec x
	\end{align*}
\end{definition}

A linear space / vector space is a set with a linear structure over a field on the set.
In normal textbook, a linear space will be defined as follows:

\begin{corollary}[Linear Spaces]
	A linear space over a field $\mathbb{k}$ is a set $V$ equipped with two operations: vector addition $+: V \times V \to V$ and scalar multiplication $\cdot : \mathbb{k} \times V \to V$, satisfying the following axioms for all $\vec u, \vec v, \vec w \in V$ and $\alpha, \beta \in \mathbb{k}$:
	\begin{center}
		\begin{tabularx}{\textwidth}{XX}
			\toprule
			\textbf{Axiom} & \textbf{Statement} \\
			\midrule
			1. Associativity of addition & $(\vec u + \vec v) + \vec w = \vec u + (\vec v + \vec w)$ \\
			2. Existence of additive identity & $\exists \vec 0 \in V$ such that $\forall \vec u \in V$, $\vec u + \vec 0 = \vec u$ \\
			3. Existence of additive inverses & $\forall \vec u \in V$, $\exists -\vec u \in V$ such that $\vec u + (-\vec u) = \vec 0$ \\
			4. Commutativity of addition & $\vec u + \vec v = \vec v + \vec u$ \\
			5. Distributivity of scalar multiplication with respect to vector addition & $\alpha (\vec u + \vec v) = \alpha \vec u + \alpha \vec v$ \\
			6. Distributivity of scalar multiplication with respect to field addition & $(\alpha + \beta) \cdot = \alpha \cdot + \beta \cdot$ \\
			7. Compatibility of scalar multiplication with field multiplication & $(\alpha \beta) \cdot = (\alpha \cdot) \circ (\beta \cdot)$ \\
			8. Identity element of scalar multiplication & $\mathbb{k} \ni 1 \mapsto (1\cdot : x \mapsto x) \in \End(V)$ \\
			\bottomrule
		\end{tabularx}
	\end{center}
\end{corollary}
\begin{remark}
	The first four axioms ensure that $(V, +)$ is an abelian group, while the fifth axiom describes the distributivity inside $\End(A)$ and the last three axioms describe the ring homomorphism.
\end{remark}

\begin{example}
	$\mathbb{k}$ is a linear space over itself with the usual addition and multiplication operations.
	\begin{align*}
		\cdot : \mathbb{k} \times \mathbb{k} &\to \mathbb{k} \\
		(\alpha,\beta) &\mapsto \alpha \beta
	\end{align*}
	The first $\mathbb{k}$ is the field acting on the second $\mathbb{k}$, which is the abelian group.
\end{example}

\begin{example}
	Let $X$ be a set and $\mathbb{k}$ be a field.
	($f$ is a set map)
	\begin{align*}
		\mathbb{k}[[X]] = \Map(X, \mathbb{k}) &\overset{\mathrm{def}}{=\joinrel=} \text{the set of all $\mathbb{k}$-valued functions on } X \\
		&=\joinrel= \left\{ f : X \to \mathbb{k} \right\}
	\end{align*}
	$\mathbb{k}[[X]]$ is a linear space over $\mathbb{k}$ with the following operations defined pointwise:
	\begin{align*}
		+ : \mathbb{k}[[X]] \times \mathbb{k}[[X]] &\to \mathbb{k}[[X]] \\
		(f,g) &\mapsto (f+g: x \mapsto f(x) + g(x)) \qquad f + g : X \to \mathbb{k} \\
		\cdot : \mathbb{k} \times \mathbb{k}[[X]] &\to \mathbb{k}[[X]] \\
		(\alpha,f) &\mapsto (\alpha f: x \mapsto \alpha f(x)) \qquad \alpha f : X \to \mathbb{k}
	\end{align*}
\end{example}

\begin{example}
	Let $X$ be a set and $\mathbb{k}$ be a field.
	\begin{align*}
		\mathbb{k}[X] = \text{Map}_{\text{fin}}(X, \mathbb{k}) &\overset{\mathrm{def}}{=\joinrel=} \text{the set of all finitely supported $\mathbb{k}$-valued functions on } X \\
		&=\joinrel= \left\{ f : X \to \mathbb{k} \mid f \text{ is finitely supported} \right\}
	\end{align*}
	$\mathbb{k}[X]$ is a linear space over $\mathbb{k}$ as $\mathbb{k}[X] \subseteq \mathbb{k}[[X]]$ and the operations are defined pointwise as in the previous example.

	$f: X \to \mathbb{k}$ is finitely supported if the set $\{ x \in X \mid f(x) \neq 0 \}$ is finite or $f(x) \neq 0$ for only finitely many $x \in X$.
\end{example}

\begin{example}
	Let $t$ be a formal variable.
	Then $\mathbb{k}[[t]] \overset{\mathrm{def}}{=\joinrel=} \mathbb{k}[[\{ 1, t, t^2, \cdots \}]] = \sum_{n = 0}^\infty a_n t^n$ is the set of all formal power series in $t$ with coefficients in $\mathbb{k}$ and $\mathbb{k}[t] \overset{\mathrm{def}}{=\joinrel=} \mathbb{k}[\{ 1, t, t^2, \cdots \}] = \sum_{n = 0}^N a_n t^n$ is the set of all polynomials in $t$ with coefficients in $\mathbb{k}$.
	Both $\mathbb{k}[[t]]$ and $\mathbb{k}[t]$ are linear spaces over $\mathbb{k}$.
\end{example}

There are another names for $\mathbb{k}[X]$ and $\mathbb{k}[[X]]$: Polynomial ring and Formal Power Series ring, respectively.

\begin{example}
	Let $n$ be a positive integer and $\mathbb{k}$ be a field.
	Then
	\[
		\mathbb{k}^n \overset{\mathrm{def}}{=\joinrel=} \left\{
		\begin{bmatrix}
			c_1 \\
			\vdots \\
			c_n
		\end{bmatrix}
		\;\middle|\; c_i \in \mathbb{k}
		\right\}
	\]
	is the set of all \emph{column matrices}\index{column matrices} with $n$ entries in $\mathbb{k}$.
	Elements in $\mathbb{k}^n$ are written as $\vec x$ and are called \emph{column vectors}\index{column vectors}. $\mathbb{k}^n$ is a linear space over $\mathbb{k}$ with the following operations defined entrywise:
	\begin{align*}
		+ : \mathbb{k}^n \times \mathbb{k}^n &\to \mathbb{k}^n \\
		(\vec a, \vec b) &\mapsto \vec a + \vec b = \begin{bmatrix}
			a_1 + b_1 \\
			\vdots \\
			a_n + b_n
		\end{bmatrix} \\
		\cdot : \mathbb{k} \times \mathbb{k}^n &\to \mathbb{k}^n \\
		(\alpha, \vec a) &\mapsto \alpha \vec a = \begin{bmatrix}
			\alpha a_1 \\
			\vdots \\
			\alpha a_n
		\end{bmatrix}
	\end{align*}
	$\mathbb{k}^n$ is a linear space over $\mathbb{k}$ automatically as $\mathbb{k}$ is a linear space over itself.
\end{example}
